<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>wzwan</title>
        <link>https://wzwan-developer.github.io/</link>
        <description>Recent content on wzwan</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>wzwan</copyright><atom:link href="https://wzwan-developer.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Targetless Extrinsic Calibration of Multiple Small FoV LiDARs and Cameras using Adaptive Voxelization</title>
        <link>https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/</link>
        <pubDate>Wed, 11 Sep 2024 09:05:20 +0800</pubDate>
        
        <guid>https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/</guid>
        <description>&lt;img src="https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/lidar_voxel.png" alt="Featured image of post Targetless Extrinsic Calibration of Multiple Small FoV LiDARs and Cameras using Adaptive Voxelization" /&gt;&lt;h2 id=&#34;文章内容&#34;&gt;文章内容
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;            With adaptive voxelization, we can obtain a set of voxels of different sizes. Each voxel contains points that are roughly on a plane and creates a planar constraint for all LiDAR poses that  have points in this voxel. More specifically, considering the $l-th$ voxel consisting of a group of points $\mathcal{P}_{l}={_{}^{G}P_{L_{i},t_{j}}}$ scanned by $L_{i} \in \mathcal{L}$ at times $t_{j} \in \mathcal{T}$. We define a point cloud consistency  indicator $_{c_{l}}(_{L_{i}}^{G}T_{t_{j}})$ which forms a factor on $\mathcal{S}$ and $\mathcal{E}_{L}$ shown in Fig. 4(a). Then, the base LiDAR trajectory and extrinsic are estimated by optimizing the factor graph. A natural choice   for the consistency indicator $c_{l}(\cdot)$ would be the summed Euclidean distance between each $_{}^{G}P_{L_{i},t_{j}}$ the plane to be estimated (see Fig. 4(b)). Taking account of all such indicators within the voxel map, we could formulate the problem as &lt;/p&gt;
$$\arg\min_{{\mathcal{S},\mathcal{E}_{L},\mathbf{n}_{l},\mathbf{q}_{l}}}\sum_{l}\underbrace{{\left(\frac{1}{N_{l}}\sum_{k=1}^{{N_{l}}}\left(\mathbf{n}_{l}^{T}\left(^{G}\mathbf{p}_{k}-\mathbf{q}_{l}\right)\right)^{2}\right)}}_{{l\mathrm{-th~factor}}}$$&lt;p&gt;， where $_{}^{G}p_{k}\in \mathcal{P}_{l}$, $N_{l}$ is the total number of points in $\mathcal{P}_{l}$, $n_{l}$ is the normal vector of the plane and $q_{l}$ is a point on this plane.
&lt;img src=&#34;https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/graph1.png&#34;
	width=&#34;1976&#34;
	height=&#34;830&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/graph1_hu4057890427085875171.png 480w, https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/graph1_hu18239084144532588545.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Fig.4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;238&#34;
		data-flex-basis=&#34;571px&#34;
	
&gt;
Fig.4 :(a) The $l-th$ factor item relating to $\mathcal{S}$ and $\mathcal{E}_{L}$ with $L_{i} \in \mathcal{L}$ and $t_{j} \in \mathcal{T}$ . (b) The distance from the point $_{}^{G}p_{k}$ to the plane $\pi$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;         通过自适应体素化，我们可以获得一组不同大小的体素。每个体素包含大致在一个平面上的点，并为所有包含在此体素内的雷达姿态创建一个平面约束。更具体地说，考虑由$L_{i} \in \mathcal{L}$在时刻$t_{j} \in \mathcal{T}$扫描的一组点组成的第$l$个体素。我们定义了一个点云一致性指标$_{c_{l}}(_{L_{i}}^{G}T_{t_{j}})$ ，它在图4(a)中形成了$\mathcal{S}$ 和 $\mathcal{E}_{L}$上的因子。然后，通过优化因子图来估计基准雷达的轨迹和外参。对于一致性指标$c_{l}(\cdot)$的一个自然选择是计算每个$_{}^{G}P_{L_{i},t_{j}}$到平面的欧几里得距离之和（见图4(b)）。考虑到体素图中所有这样的指标，我们可以将问题表述为&lt;/p&gt;
$$\arg\min_{{\mathcal{S},\mathcal{E}_{L},\mathbf{n}_{l},\mathbf{q}_{l}}}\sum_{l}\underbrace{{\left(\frac{1}{N_{l}}\sum_{k=1}^{{N_{l}}}\left(\mathbf{n}_{l}^{T}\left(^{G}\mathbf{p}_{k}-\mathbf{q}_{l}\right)\right)^{2}\right)}}_{{l\mathrm{-th~factor}}}$$&lt;p&gt;，其中 $_{}^{G}p_{k}\in \mathcal{P}_{l}$，$N_{l}$ 是 $\mathcal{P}_{l}$中所有点的总点数, $n_{l}$ 是平面的法向量， $q_{l}$ 是平面中的一点。
&lt;img src=&#34;https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/graph1.png&#34;
	width=&#34;1976&#34;
	height=&#34;830&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/graph1_hu4057890427085875171.png 480w, https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/graph1_hu18239084144532588545.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;图4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;238&#34;
		data-flex-basis=&#34;571px&#34;
	
&gt;
Fig.4 :(a) 第$l$ 个因子项，涉及$\mathcal{S}$ 和 $\mathcal{E}_{L}$，其中  $L_{i} \in \mathcal{L}$ 且 $t_{j} \in \mathcal{T}$ 。 (b)点 $_{}^{G}p_{k}$到平面$\pi$的距离.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;              It is noticed that the optimization variables $(n_{l}, q_{l})$ in (2) could be analytically solved (see Appendix A) and the resultant cost function (3) is over the LiDAR pose $_{L_{i}}^{G}T_{t_{j}}$ (hence the base LiDAR trajectory $\mathcal{S}$ and extrinsic $\mathcal{E}_{L}$) only, as follows &lt;/p&gt;
$$ \arg\min_{\mathcal{S},\mathcal{E}_{L}}\sum_{l}^{}\lambda_{3}(A_{l}) $$&lt;p&gt; where $\lambda_{3}(A_{l})$ denotes the minimal eigenvalue of matrix $A_{l}$ defined as &lt;/p&gt;
$$ A_{l}=\frac{1}{N_{l}}\sum_{k=1}^{N_{l}}{_{}^{G}p_{k} \cdot_{}^{G}p_{k}^{T}-q_{l}^{\ast}\cdot {q_{l}^{\ast}}^{T}},q_{l}^{\ast} =\frac{1}{N_{l}}\sum_{k=1}^{N_{l}}{_{}^{G}p_{k}}$$&lt;p&gt; . To allow efficient optimization in (3), we derive the closedform derivatives w.r.t the optimization variable $x$ up to secondorder (the detailed derivation from (3) to (5) is elaborated in Appendix B):&lt;/p&gt;
$$\lambda_3(\mathbf{x}\boxplus\delta\mathbf{x})\approx\lambda_3(\mathbf{x})+\mathbf{\bar{J}}\delta\mathbf{x}+\frac12\delta\mathbf{x}^T\mathbf{\bar{H}}\delta\mathbf{x}$$&lt;p&gt; ,where $\bar{J}$ is the Jacobian matrix, and $\bar{H}$  is the Hessian matrix.The $\delta{x}$  is a small perturbation of the optimization variable $x$:&lt;/p&gt;
$$\mathbf{x}=[\underbrace{\cdots_{L_{0}}^{G}\mathbf{R}_{t_{j}}\quad L_{0}^{G}\mathbf{t}_{t_{j}}\cdots}_{\mathcal{S}}\underbrace{\cdots_{L_{i}}^{L_{0}}\mathbf{R}_{L_{i}}^{L_{0}}\mathbf{t}\cdots}_{\mathcal{E}_{L}}]$$&lt;p&gt; .Then the optimal $x^{\ast}$ could be determined by iteratively solving (6) with the LM method and updating the $\delta{x}$ to $x$.
&lt;/p&gt;
$$(\bar{\mathbf{H}}+\mu\mathbf{I}) \delta\mathbf{x}=-\bar{\mathbf{J}}^T$$&lt;/blockquote&gt;
&lt;p&gt;         注意到优化变量$(n_{l}, q_{l})$在方程(2)中可以解析求解（详见附录A）,由此得到的损失函数(3)仅关于雷达姿态$_{L_{i}}^{G}T_{t_{j}}$(即基准雷达轨迹$\mathcal{S}$和外参$\mathcal{E}_{L}$),如下所示：
&lt;/p&gt;
$$ \arg\min_{\mathcal{S},\mathcal{E}_{L}}\sum_{l}^{}\lambda_{3}(A_{l}) $$&lt;p&gt;
其中$\lambda_{3}(A_{l})$表示矩阵 $A_{l}$的最小特征值， $A_{l}$定义为&lt;/p&gt;
$$ A_{l}=\frac{1}{N_{l}}\sum_{k=1}^{N_{l}}{_{}^{G}p_{k} \cdot_{}^{G}p_{k}^{T}-q_{l}^{\ast}\cdot {q_{l}^{\ast}}^{T}},q_{l}^{\ast} =\frac{1}{N_{l}}\sum_{k=1}^{N_{l}}{_{}^{G}p_{k}}$$&lt;p&gt;
。为了使式(3)中的优化高效，我们推导了优化变量$x$的二阶闭式导数(从(3)到(5)的详细推导见附录B)：
&lt;/p&gt;
$$\lambda_3(\mathbf{x}\boxplus\delta\mathbf{x})\approx\lambda_3(\mathbf{x})+\mathbf{\bar{J}}\delta\mathbf{x}+\frac12\delta\mathbf{x}^T\mathbf{\bar{H}}\delta\mathbf{x}$$&lt;p&gt;
。其中$\bar{J}$是雅可比矩阵，$\bar{H}$是海森矩阵。$\delta{x}$是优化变量$x$的小扰动：
&lt;/p&gt;
$$\mathbf{x}=[\underbrace{\cdots_{L_{0}}^{G}\mathbf{R}_{t_{j}}\quad L_{0}^{G}\mathbf{t}_{t_{j}}\cdots}_{\mathcal{S}}\underbrace{\cdots_{L_{i}}^{L_{0}}\mathbf{R}_{L_{i}}^{L_{0}}\mathbf{t}\cdots}_{\mathcal{E}_{L}}]$$&lt;p&gt;
。然后，最优解$x^{\ast}$可以通过迭代求解公式(6)并使用LM的方法更新$\delta{x}$到$x$来确定。
&lt;/p&gt;
$$(\bar{\mathbf{H}}+\mu\mathbf{I}) \delta\mathbf{x}=-{\bar{\mathbf{J}}}^T$$&lt;h2 id=&#34;理论推导&#34;&gt;理论推导
&lt;/h2&gt;&lt;h3 id=&#34;损失函数的降维&#34;&gt;损失函数的降维
&lt;/h3&gt;&lt;h4 id=&#34;推导一balm论文的思路&#34;&gt;推导一：BALM论文的思路
&lt;/h4&gt;$$\arg\min_{{\mathcal{S},\mathcal{E}_{L},\mathbf{n}_{l},\mathbf{q}_{l}}}\sum_{l}\underbrace{{\left(\frac{1}{N_{l}}\sum_{k=1}^{{N_{l}}}\left(\mathbf{n}_{l}^{T}\left(^{G}\mathbf{p}_{k}-\mathbf{q}_{l}\right)\right)^{2}\right)}}_{{l\mathrm{-th~factor}}}$$&lt;p&gt;
观察上式可知，由于对于平面参数$\pi=(n_{l},q_{l})$的依赖，原始误差函数优化的维度很高。因为平面参数$(n_{l},q_{l})$对于不同的平面是有区别的。我们可以对它们进行优化，如下式所示：
&lt;/p&gt;
$$\arg\min_{{\mathcal{S},\mathcal{E}_{L}}}\sum_{l}{\left(\min_{n_{l},q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( n_{l}^{T}\left( {^{G}p_{k}-q_{l}}\right)\right)^{2}}} \right)} $$&lt;p&gt;
在$(n_{l},q_{l})$两个平面参数优化过程中可以优先优化$q_{l}$，再优化$n_{l}$，下式为优化$q_{l}$。
&lt;/p&gt;
$$\begin{align}\arg\min_{n_{l},q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( n_{l}^{T}\left( {^{G}p_{k}-q_{l}}\right)\right)^{2}}} \\
= \arg\min_{n_{l}}\left( \min_{q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( n_{l}^{T}\left( {^{G}p_{k}-q_{l}}\right)\right)^{2}}}\right)\\
=\arg\min_{n_{l}}n_{l}^{T}\left( \min_{q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}}\right)^{2}}}\right)n_{l}
 \end{align}$$&lt;p&gt;
继续简化关于$q_{l}$参数的优化
&lt;/p&gt;
$$\begin{align}\arg\min_{q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}}\right)^{2}}}\\
 =\min_{q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}}\right)\left( {^{G}p_{k}-q_{l}}\right)^{T}}}\end{align}$$&lt;p&gt;
需要求当上式最小化是$q_{l}$的最优解，我们需要找到上式梯度为0的$q_{l}$的值，因此我们对上式求关于$q_{l}$的导数，如下所示：
&lt;/p&gt;
$$\begin{align}\frac{\partial}{\partial{q_{l}}} \left( {\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}}\right)\left( {^{G}p_{k}-q_{l}}\right)^{T}}}\right) \\
={\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}}  {-({^{G}p_{k}-q_{l})}^{T}}-\left( {^{G}p_{k}-q_{l}}\right)}}\\
= {\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}}  {-2({^{G}p_{k}-q_{l})}}}}\\
=-{\frac{2}{N_{l}}{\sum_{k=1}^{N_{l}}  {({^{G}p_{k}-q_{l})}}}}
\end{align}$$&lt;p&gt;
令上式等于0：
&lt;/p&gt;
$$\begin{align}-{\frac{2}{N_{l}}{\sum_{k=1}^{N_{l}}  {({^{G}p_{k}}-q_{l})}}}=0 \\
 \sum_{k=1}^{N_{l}}{({^{G}p_{k}-q_{l}})} =0\\
 \sum_{k=1}^{N_{l}}{^{G}p_{k}}-N_{l}q_{l}=0\\
 N_{l}q_{l}=\sum_{k=1}^{N_{l}}{^{G}p_{k}}\\
 q_{l}=\frac{1}{N_{l}}\sum_{k=1}^{N_{l}}{^{G}p_{k}}
 \end{align}$$&lt;p&gt;
由此可知，最优的$q_{l}$是所有$^{G}p_{k}$的均值$q_{l}^{\ast}$,接下来继续简化$n_{l}$
&lt;/p&gt;
$$\begin{align}\arg\min_{n_{l}}n_{l}^{T}\left( \min_{q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}}\right)^{2}}}\right)n_{l}\\
=\arg\min_{n_{l}}n_{l}^{T}\left( {\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}^{\ast}}\right)^{2}}}\right)n_{l}\end{align}$$&lt;p&gt;
其中，&lt;/p&gt;
$${\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}^{\ast}}\right)^{2}}}\\
\Leftrightarrow \frac{1}{N_{l}}\sum_{k=1}^{N_{l}}{_{}^{G}p_{k} \cdot_{}^{G}p_{k}^{T}-q_{l}^{\ast}\cdot {q_{l}^{\ast}}^{T}} $$&lt;p&gt;
需要注意的是上式中$q_{l}^{\ast} \cdot_{}^{G}p_{k}^{T}$与$_{}^{G}p_{k}\cdot{q_{l}^{\ast}}^{T}$两个交叉项在最小化过程中不会对优化结果产生影响，所以两式等价。
所以，带入&lt;/p&gt;
$$ A_{l}=\frac{1}{N_{l}}\sum_{k=1}^{N_{l}}{_{}^{G}p_{k} \cdot_{}^{G}p_{k}^{T}-q_{l}^{\ast}\cdot {q_{l}^{\ast}}^{T}},q_{l}^{\ast} =\frac{1}{N_{l}}\sum_{k=1}^{N_{l}}{_{}^{G}p_{k}}$$$$\begin{align}\arg\min_{n_{l}}n_{l}^{T}\left( {\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}^{\ast}}\right)^{2}}}\right)n_{l}\\
=\arg\min_{n_{l}}n_{l}^{T}A_{l}n_{l}\end{align}$$&lt;p&gt;
根据瑞利商定理，对矩阵$M$满足如下性质：
&lt;/p&gt;
$$\lambda_{min}(M)\le \frac{x^{T}Mx}{x^{T}x}\le\lambda_{max}(M),\forall{x}\ne0$$&lt;p&gt;
则关于误差函数，取得最小化，变量$(n_{l}, q_{l}$的优化为公式&lt;/p&gt;
$$\begin{align}\arg\min_{n_{l}}n_{l}^{T}\left( \min_{q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( {^{G}p_{k}-q_{l}}\right)^{2}}}\right)n_{l}\\
=\lambda_{min}(A_{l})\\
=\lambda_{3}(A_{l})\end{align}$$&lt;p&gt;
将其带入&lt;/p&gt;
$$\begin{align}\arg\min_{{\mathcal{S},\mathcal{E}_{L}}}\sum_{l}{\left(\min_{n_{l},q_{l}}{\frac{1}{N_{l}}{\sum_{k=1}^{N_{l}} \left( n_{l}^{T}\left( {^{G}p_{k}-q_{l}}\right)\right)^{2}}} \right)}\\
=\arg\min_{{\mathcal{S},\mathcal{E}_{L}}}\sum_{l}{\lambda_{3}(A_{l})}\end{align}$$&lt;h4 id=&#34;推导二该论文中的推导思路&#34;&gt;推导二：该论文中的推导思路
&lt;/h4&gt;&lt;p&gt;&lt;span style=&#34;color:red;&#34;&gt;后续补充！&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;二阶闭式导数的推导&#34;&gt;二阶闭式导数的推导
&lt;/h3&gt;&lt;h4 id=&#34;推导一balm论文的思路-1&#34;&gt;推导一：BALM论文的思路
&lt;/h4&gt;&lt;p&gt;首先引入BALM中两个定理，如下所示:&lt;/p&gt;
&lt;h5 id=&#34;定理1&#34;&gt;定理1
&lt;/h5&gt;&lt;blockquote&gt;
&lt;p&gt;已知：&lt;/p&gt;
$$\mathbf{A}=\frac1N\sum_{i=1}^N\left(\mathbf{p}_i-\bar{\mathbf{p}}\right)\left(\mathbf{p}_i-\bar{\mathbf{p}}\right)^T,\bar{\mathbf{p}}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{p}_{i}$$&lt;p&gt;
对于一组点$p_{i}\left(i=1,\dots,\right)$和定义的协方差矩阵$A$,假设$A$具有对应于特征向量的$u_{k}\left(k=1,2,3\right)$,则有&lt;/p&gt;
$$\frac{\partial\lambda_k}{\partial\mathbf{p}_i}=\frac2N(\mathbf{p}_i-\bar{\mathbf{p}})^T\mathbf{u}_k\mathbf{u}_k^T$$&lt;p&gt;其中,$\bar{p}$是$N$个点的均值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;证明如下&lt;/strong&gt;：假设点$p_i= [x_{i} , y_{i} , z_{i}]^{T}$以及对应的特征向量矩阵$U={[u_{1} , u_{2} , u_{3} ]}^{T}$。进一步定义$p$是$p_{i}$的一个元素，$p$是$x_{i},y_{i},z_{i}$中其中一个。协方差矩阵$A$可以分解为
$\Lambda=U^{T}AU$,其中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Lambda$是对角矩阵，其对角线元素是特征值,
$\lambda_{1},\lambda_{2},\lambda_{3}$。&lt;/li&gt;
&lt;li&gt;$U$是一个正交矩阵，其列是$A$的特征向量$u_{1},u_{2},u_{3}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对$\Lambda$求导可得下式：
&lt;/p&gt;
$$
\frac{\partial{}\Lambda}{\partial{p}}=\left( \frac{\partial{U}}{\partial{p}}\right)^{T}AU+U^{T}\left(\frac{\partial{A}}{\partial{p}}\right)U+U^{T}A\left(\frac{\partial{U}}{\partial{p}}\right)\tag{式1}
$$&lt;p&gt;
又因为$\Lambda=U^{T}AU$,可以推导得到&lt;/p&gt;
$$U^{T}A=\Lambda{}U^{T},AU=U\Lambda\tag{式2}$$&lt;p&gt;,将式1带入式2，可得：
&lt;/p&gt;
$$
\frac{\partial{}\Lambda}{\partial{p}}=\left( \frac{\partial{U}}{\partial{p}}\right)^{T}U\Lambda+U^{T}\left(\frac{\partial{A}}{\partial{p}}\right)U+\Lambda{}U^{T}\left(\frac{\partial{U}}{\partial{p}}\right)\tag{式3}
$$&lt;p&gt;
又因为：
&lt;/p&gt;
$$
U^{T}U=I\\
\Longrightarrow {U^{T}\frac{\partial{U}}{\partial{p}}+\left(\frac{\partial{U}}{\partial{p}}\right)^{T}U=0}\tag{式4}
$$&lt;p&gt;
由此可知，$U^{T}\frac{\partial{U}}{\partial{p}}$是一个&lt;a class=&#34;link&#34; href=&#34;https://baike.baidu.com/item/%E5%8F%8D%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5/9063240&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;反对称矩阵&lt;/a&gt;，其对角线元素和0。
&lt;img src=&#34;https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/skew-symmetric-matrix.png&#34;
	width=&#34;1640&#34;
	height=&#34;268&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/skew-symmetric-matrix_hu12682485345853843236.png 480w, https://wzwan-developer.github.io/p/targetless-extrinsic-calibration-of-multiple-small-fov-lidars-and-cameras-using-adaptive-voxelization/skew-symmetric-matrix_hu16826235980269994892.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;反对称矩阵的定义&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;611&#34;
		data-flex-basis=&#34;1468px&#34;
	
&gt;
因此可得:
&lt;/p&gt;
$$
\left( \frac{\partial{U}}{\partial{p}}\right)^{T}U\Lambda+\Lambda{}U^{T}\left(\frac{\partial{U}}{\partial{p}}\right)=0\tag{式5}
$$&lt;p&gt;
将式5带入式3中，可得：
&lt;/p&gt;
$$
\frac{\partial{}\Lambda}{\partial{p}}=U^{T}\left(\frac{\partial{A}}{\partial{p}}\right)U\tag{式6}
$$&lt;p&gt;
又因为$\lambda_{k}={u_{k}}^{T}Au_{k}$,代入式6可得：
&lt;/p&gt;
$$
\frac{\partial{}\lambda_{k}}{\partial{p}}={u_{k}}^{T}\left(\frac{\partial{A}}{\partial{p}}\right)u_{k}=\frac{\partial{{u_{k}}^{T}Au_{k}}}{\partial{p}}\tag{式7}
$$&lt;p&gt;
此时将标量$p$换回为向量$p_{i}$,得到下式
&lt;/p&gt;
$$
\frac{\partial{}\lambda_{k}}{\partial{p_{i}}}=\begin{bmatrix} 
\frac{\partial{{u_{k}}^{T}Au_{k}}}{\partial{x_{i}}}&amp;
\frac{\partial{{u_{k}}^{T}Au_{k}}}{\partial{y_{i}}}&amp;
\frac{\partial{{u_{k}}^{T}Au_{k}}}{\partial{z_{i}}}
\end{bmatrix}=\frac{\partial{{u_{k}}^{T}Au_{k}}}{\partial{p_{i}}}\tag{式8}
$$&lt;p&gt;
带入矩阵$A$和$\bar{p}$的定义可得：
&lt;/p&gt;
$$
\begin{align}
\frac{\partial{}\lambda_{k}}{\partial{p_{j}}}=\frac1N\sum_{i=1}^N{\frac{\partial{{u_{k}}^{T}\left({p}_j-\bar{p}\right)\left(p_j-\bar{p}\right)^Tu_{k}}}{\partial{p_{i}}}}\\
=\frac1N\sum_{j=1}^N{\frac{\partial{\left({p}_j-\bar{p}\right)^{T}{u_{k}}{u_{k}}^{T}\left(p_j-\bar{p}\right)}}{\partial{p_{i}}}}\\
=\frac1N\sum_{j=1}^N{\frac{\partial\left(\left({u_{k}}^{T}\left(p_j-\bar{p}\right)\right)^{T}\left({u_{k}}^{T}\left(p_j-\bar{p}\right)\right)\right)}{\partial{p_{i}}}}\\
=\frac1N\sum_{j=1}^N{2{\left(p_j-\bar{p}\right)^{T}u_{k}}\frac{\partial{u_{k}}^{T}\left(p_j-\bar{p}\right)}{\partial{p_{i}}}}\\
=\frac2N\sum_{j=1}^N{{\left(p_j-\bar{p}\right)^{T}u_{k}}\frac{\partial{u_{k}}^{T}\left(p_j-\bar{p}\right)}{\partial{p_{i}}}}\\
=\frac2N\sum_{j=1}^N{{\left(p_j-\bar{p}\right)^{T}u_{k}}\frac{\partial{u_{k}}^{T}\left(p_j-\left(\frac{1}{N}\sum_{i=1}^{N}p_{i}\right)\right)}{\partial{p_{i}}}}\\
=\frac2N\sum_{j=1}^N{{\left(p_j-\bar{p}\right)^{T}u_{k}}\frac{\partial{u_{k}}^{T}\left(p_j-\left(\frac{1}{N}\sum_{i=1}^{N}p_{i}\right)\right)}{\partial{p_{i}}}}\\
=\frac2N{{\left(p_i-\bar{p}\right)^{T}u_{k}}{u_{k}}^{T}\left(I-\frac{1}{N}I\right)}+\\\frac2N\sum_{j=1,j\ne{i}}^N\left(p_j-\bar{p}\right)^{T}u_{k}{u_{k}}^{T}\left(-\frac{1}{N}I\right)\\
=\frac2N{{\left(p_i-\bar{p}\right)^{T}u_{k}}{u_{k}}^{T}\left(I-\frac{1}{N}I\right)}+\\\frac2Nu_{k}{u_{k}}^{T}\left(-\frac{1}{N}I\right)\sum_{j=1,j\ne{i}}^N\left(p_j-\bar{p}\right)^{T}\\
=\frac2N{{\left(p_i-\bar{p}\right)^{T}u_{k}}{u_{k}}^{T}\left(I-\frac{1}{N}I\right)}+\\\frac2Nu_{k}{u_{k}}^{T}\left(-\frac{1}{N}I\right)\left(\left(\sum_{j=1}^{N}(p_{j}-\bar{p})^{T}\right)-\left(p_i-\bar{p}\right)^{T}\right)\\
=\frac2N{{\left(p_i-\bar{p}\right)^{T}u_{k}}{u_{k}}^{T}\left(I-\frac{1}{N}I\right)}+\\\frac2Nu_{k}{u_{k}}^{T}\left(-\frac{1}{N}I\right)\left(0-\left(p_i-\bar{p}\right)^{T}\right)\\
=\frac2N{{\left(p_i-\bar{p}\right)^{T}u_{k}}{u_{k}}^{T}}
\end{align}\tag{式9}
$$&lt;h5 id=&#34;定理2&#34;&gt;定理2
&lt;/h5&gt;&lt;blockquote&gt;
&lt;p&gt;已知：&lt;/p&gt;
$$\mathbf{A}=\frac1N\sum_{i=1}^N\left(\mathbf{p}_i-\bar{\mathbf{p}}\right)\left(\mathbf{p}_i-\bar{\mathbf{p}}\right)^T,\bar{\mathbf{p}}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{p}_{i}$$&lt;p&gt;
对于一组点$p_{i}\left(i=1,\dots,\right)$和定义的协方差矩阵$A$,假设$A$具有对应于特征向量的$u_{k}\left(k=1,2,3\right)$,则有&lt;/p&gt;
$$\begin{aligned}\frac{\partial^{2}\lambda_{k}}{\partial\mathbf{p}_{j}\partial\mathbf{p}_{i}}=\begin{cases}\frac{2}{N}\biggl(\frac{N-1}{N}\mathbf{u}_{k}\mathbf{u}_{k}^{T}+\mathbf{u}_{k}(\mathbf{p}_{i}-\bar{\mathbf{p}})^{T}\mathbf{U}\mathbf{F}_{k}^{\mathbf{p}_{j}}\\\\+\mathbf{U}\mathbf{F}_{k}^{\mathbf{p}_{j}}\biggl(\mathbf{u}_{k}^{T}(\mathbf{p}_{i}-\bar{\mathbf{p}})\biggr)\biggr),\quad i=j\\\\\frac{2}{N}\biggl(-\frac{1}{N}\mathbf{u}_{k}\mathbf{u}_{k}^{T}+\mathbf{u}_{k}(\mathbf{p}_{i}-\bar{\mathbf{p}})^{T}\mathbf{U}\mathbf{F}_{k}^{\mathbf{p}_{j}}\\\\+\mathbf{U}\mathbf{F}_{k}^{\mathbf{p}_{j}}\biggl(\mathbf{u}_{k}^{T}(\mathbf{p}_{i}-\bar{\mathbf{p}})\biggr)\biggr),\quad i\neq j\end{cases}\end{aligned}$$&lt;p&gt; &lt;/p&gt;
$$\mathbf{F}_{k}^{\mathbf{P}_{j}}=\begin{bmatrix}\mathbf{F}_{1,k}^{\mathbf{P}_{j}}\\\mathbf{F}_{2,k}^{\mathbf{P}_{j}}\\\mathbf{F}_{3,k}^{\mathbf{P}_{j}}\end{bmatrix}\in\mathbb{R}^{3\times3},\quad\mathbf{U}=\begin{bmatrix}\mathbf{u}_{1}&amp;\mathbf{u}_{2}&amp;\mathbf{u}_{3}\end{bmatrix}$$&lt;p&gt; &lt;/p&gt;
$$\left.\mathbf{F}_{m,n}^{\mathbf{P}_{j}}=\left\{\begin{matrix}\frac{(\mathbf{p}_{j}-\bar{\mathbf{p}})^{T}}{N(\lambda_{n}-\lambda_{m})}(\mathbf{u}_{m}\mathbf{u}_{n}^{T}+\mathbf{u}_{n}\mathbf{u}_{m}^{T}),m\neq n\\\mathbf{0}_{1\times3}&amp;,m=n\end{matrix}\right.\right.$$&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;证明如下&lt;/strong&gt;：
已知协方差矩阵可被分解，且设$q$是点$p_{i}$的一个标量$x_{i},y_{i},z_{i}$之一，如下所示：
&lt;/p&gt;
$$\mathbf{A}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{T} \tag{式1}$$&lt;p&gt;
首先对其求关于$q$的导数：
&lt;/p&gt;
$$\frac{\partial\mathbf{A}}{\partial q}=\frac{\partial}{\partial q}\left(\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{T}\right)\tag{式2}$$&lt;p&gt;
根据链式法则，右侧可以写为：
&lt;/p&gt;
$${\frac{\partial\mathbf{A}}{\partial q}}={\frac{\partial\mathbf{U}}{\partial q}}\mathbf{\Lambda}\mathbf{U}^{T}+\mathbf{U}{\frac{\partial\mathbf{\Lambda}}{\partial q}}\mathbf{U}^{T}+\mathbf{U}\mathbf{\Lambda}{\frac{\partial\mathbf{U}^{T}}{\partial q}} $$&lt;p&gt;
由于$\frac{\partial{U}^{T}}{\partial{q}}=\left({\frac{\partial\mathbf{U}}{\partial q}}\right)^{T}$,因此式2可以变为
&lt;/p&gt;
$$\begin{align}
\frac{\partial\mathbf{A}}{\partial q}=\frac{\partial\mathbf{U}}{\partial q}\mathbf{\Lambda}\mathbf{U}^T+\mathbf{U}\frac{\partial\mathbf{\Lambda}}{\partial q}\mathbf{U}^T+\mathbf{U}\mathbf{\Lambda}\left(\frac{\partial\mathbf{U}}{\partial q}\right)^T\\
\Longrightarrow \mathbf{U}^{T}{\frac{\partial\mathbf{A}}{\partial q}}\mathbf{U}=\mathbf{U}^{T}{\frac{\partial\mathbf{U}}{\partial q}}\mathbf{\Lambda}+{\frac{\partial\mathbf{\Lambda}}{\partial q}}+\mathbf{\Lambda}\left({\frac{\partial\mathbf{U}}{\partial q}}\right)^{T}\mathbf{U} 
\end{align}\tag{式3}$$&lt;p&gt;
设$C^{q}=U^{T}\frac{\partial\mathbf{U}}{\partial q} $,因为$C^{q}$是反对称矩阵(由定理1的证明可知),所以有以下$C^{q}+{C^{q}}^{T}=0$,即${C^{q}}^{T}=-C^{q}$。代入式3可得：
&lt;/p&gt;
$$
\mathbf{U}^{T}{\frac{\partial\mathbf{A}}{\partial q}}\mathbf{U}=\mathbf{C}^{q}\mathbf{\Lambda}+{\frac{\partial\mathbf{\Lambda}}{\partial q}}-\mathbf{\Lambda}\mathbf{C}^{q}
\tag{式4}$$&lt;p&gt;
取$C^{q}$的m行，n列可得：
&lt;/p&gt;
$$
\begin{align}
\left(\mathbf{U}^T\frac{\partial\mathbf{A}}{\partial q}\mathbf{U}\right)_{m,n}=\left(\frac{\partial\boldsymbol{\Lambda}}{\partial q}\right)_{m,n}-\left(\mathbf{\Lambda}\mathbf{C}^q-\mathbf{C}^q\mathbf{\Lambda}\right)_{m,n}\\
\Longrightarrow 
\left(\frac{\partial\boldsymbol{\Lambda}}{\partial q}\right)_{m,n}=\left(\mathbf{U}^T\frac{\partial\mathbf{A}}{\partial q}\mathbf{U}\right)_{m,n}+\left(\mathbf{\Lambda}\mathbf{C}^q-\mathbf{C}^q\mathbf{\Lambda}\right)_{m,n}\\
\Longrightarrow 
0=\left(\mathbf{U}^T\frac{\partial\mathbf{A}}{\partial q}\mathbf{U}\right)_{m,n}+\left(\mathbf{\Lambda}\mathbf{C}^q-\mathbf{C}^q\mathbf{\Lambda}\right)_{m,n}\\
\Longrightarrow 
0=u_{m}^{T}{\frac{\partial{A}}{\partial{q}}}u_{n}+\lambda_{m}C_{m,n}^{q}-C_{m,n}^{q}\lambda_{n}\\
\Longrightarrow 
C_{m,n}^{q}\left(\lambda_{m}-\lambda_{n}\right)=-u_{m}^{T}{\frac{\partial{A}}{\partial{q}}}u_{n}\\
\Longrightarrow 
\mathbf{C}_{m,n}^q=\frac{\mathbf{u}_m^T\frac{\partial\mathbf{A}}{\partial q}\mathbf{u}_n}{\lambda_n-\lambda_m}&amp;,(\lambda_{m}\ne\lambda_{n})
\end{align}
\tag{式5}$$&lt;p&gt;
取$C^{q}$的m行，m列,由于它是反对称矩阵，所以它的对角线元素为0，即：
&lt;/p&gt;
$$\begin{align}
\mathbf{C}_{m,m}^q=\mathbf{u}_m^T\frac{\partial\mathbf{u}_m}{\partial q}\\
\Longrightarrow 
{C}_{m,m}^q=0&amp;,(m=n)
\end{align}\tag{式6}$$&lt;p&gt;
接下来求解$u_{k}$对p_{j}的三个分量的微积分：
&lt;/p&gt;
$$
\begin{align}
\frac{\partial u_{k}}{\partial{q}}=\frac{\partial Ue_{k}}{\partial q}
=UU^{T}\frac{\partial U}{\partial q}e_{k}
=UC^{q}e_{k}
\end{align}
\tag{式7}
$$&lt;p&gt;
将$q$替换为$p_{j}$,可得：
&lt;/p&gt;
$$
\begin{align}
\begin{aligned}
\frac{\partial\mathbf{u}_k}{\partial\mathbf{p}_j}&amp; =\begin{bmatrix}\frac{\partial\mathbf{U}\mathbf{e}_k}{\partial x_j}&amp;\frac{\partial\mathbf{U}\mathbf{e}_k}{\partial y_j}&amp;\frac{\partial\mathbf{U}\mathbf{e}_k}{\partial z_j}\end{bmatrix} \\
&amp;=\begin{bmatrix}\mathbf{U}\mathbf{C}^{x_j}\mathbf{e}_k&amp;\mathbf{U}\mathbf{C}^{y_j}\mathbf{e}_k&amp;\mathbf{U}\mathbf{C}^{z_j}\mathbf{e}_k\end{bmatrix} \\
&amp;=\mathbf{U}\big[\mathbf{C}^{x_j}\mathbf{e}_k\quad\mathbf{C}^{y_j}\mathbf{e}_k\quad\mathbf{C}^{z_j}\mathbf{e}_k\big] \\
&amp;=\mathbf{U}\begin{bmatrix}\mathbf{C}_{1,k}^{x_j}&amp;\mathbf{C}_{1,k}^{y_j}&amp;\mathbf{C}_{1,k}^{z_j}\\\mathbf{C}_{2,k}^{x_j}&amp;\mathbf{C}_{2,k}^{y_j}&amp;\mathbf{C}_{2,k}^{z_j}\\\mathbf{C}_{3,k}^{x_j}&amp;\mathbf{C}_{3,k}^{y_j}&amp;\mathbf{C}_{3,k}^{z_j}\end{bmatrix}
\end{aligned}
\end{align}
\tag{式8}
$$&lt;h4 id=&#34;推导二该论文中的推导思路-1&#34;&gt;推导二：该论文中的推导思路
&lt;/h4&gt;&lt;p&gt;&lt;span style=&#34;color:red;&#34;&gt;后续补充！&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;定理1：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;代码详解&#34;&gt;代码详解
&lt;/h2&gt;&lt;h2 id=&#34;参考文献&#34;&gt;参考文献
&lt;/h2&gt;&lt;p&gt;[1]&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2109.06550&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;《Targetless Extrinsic Calibration of Multiple Small FoV LiDARs and Cameras using Adaptive Voxelization》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2]&lt;a class=&#34;link&#34; href=&#34;https://www.arxiv.org/pdf/2010.08215&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;《BALM: Bundle Adjustment for Lidar Mapping》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]&lt;a class=&#34;link&#34; href=&#34;http://epsilonjohn.club/2020/11/03/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/BALM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;《BALM论文阅读》——epsilonjohn的博客文章&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>如何理解概率论中的“矩”</title>
        <link>https://wzwan-developer.github.io/p/203/</link>
        <pubDate>Mon, 09 Sep 2024 11:12:48 +0800</pubDate>
        
        <guid>https://wzwan-developer.github.io/p/203/</guid>
        <description>&lt;img src="https://wzwan-developer.github.io/p/203/gaussian-normal-distribution-graph.png" alt="Featured image of post 如何理解概率论中的“矩”" /&gt;&lt;h2 id=&#34;概率论中的矩&#34;&gt;概率论中的“矩”
&lt;/h2&gt;&lt;h3 id=&#34;彩票的问题&#34;&gt;彩票的问题
&lt;/h3&gt;&lt;p&gt;假设福利彩票，每一注两元钱，且中奖的概率分布如下：
&lt;img src=&#34;https://wzwan-developer.github.io/p/203/graph1.png&#34;
	width=&#34;359&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/203/graph1_hu7074458194076008173.png 480w, https://wzwan-developer.github.io/p/203/graph1_hu11398514835930632876.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;概率分布图&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;89&#34;
		data-flex-basis=&#34;214px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中，概率的“称”如下所示：
&lt;img src=&#34;https://wzwan-developer.github.io/p/203/graph2.png&#34;
	width=&#34;613&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/203/graph2_hu1581581583894811490.png 480w, https://wzwan-developer.github.io/p/203/graph2_hu3775081096410037377.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;概率的称&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;此时我们称量一下中奖500万元：
&lt;img src=&#34;https://wzwan-developer.github.io/p/203/graph3.png&#34;
	width=&#34;613&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/203/graph3_hu9993873231620659325.png 480w, https://wzwan-developer.github.io/p/203/graph3_hu8003149627366197390.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;500万元&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;上述结果表明：不确定的500万元等价于确定的0.5元。此时将所有的中奖概率刻画上去：
&lt;img src=&#34;https://wzwan-developer.github.io/p/203/graph4.png&#34;
	width=&#34;613&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/203/graph4_hu8029704338762354472.png 480w, https://wzwan-developer.github.io/p/203/graph4_hu18134410058688724433.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;上述结果等于&lt;/p&gt;
$$1.5 = 5\times 10\% + 100\times0.5\% + 5000000\times0.000001\% $$&lt;p&gt;
结果表明一张彩票成本两元，但是期望获得的收益为1.5元，每买一张都会亏损0.5元。&lt;/p&gt;
&lt;h2 id=&#34;矩&#34;&gt;“矩”
&lt;/h2&gt;&lt;h3 id=&#34;一阶矩&#34;&gt;一阶矩
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;上述我们计算的就是概率的一阶矩，也就是期望（expectation/mean）。
&lt;/code&gt;&lt;/pre&gt;
$$
E[X]=\sum p_{i}x_{i}
$$&lt;p&gt;含义如下：
&lt;img src=&#34;https://wzwan-developer.github.io/p/203/graph5.png&#34;
	width=&#34;572&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/203/graph5_hu18168430126999916666.png 480w, https://wzwan-developer.github.io/p/203/graph5_hu8960376600484578208.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;”期望含义“&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;341px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;二阶矩&#34;&gt;二阶矩
&lt;/h3&gt;&lt;p&gt;二阶矩是广为认知的协方差矩阵$\Sigma$
&lt;/p&gt;
$$
    \Sigma=E[(X-\mu)^{2}]=\sum_{i}p_{i}(x_{i}-\mu)^{2}
$$&lt;h3 id=&#34;高阶矩&#34;&gt;高阶矩
&lt;/h3&gt;&lt;p&gt;三阶矩称为偏度，四阶矩称为峰度。各有用途但是共同的特点为称量之后才能使用。&lt;/p&gt;
&lt;h2 id=&#34;参考链接&#34;&gt;参考链接
&lt;/h2&gt;&lt;p&gt;[1]&lt;a class=&#34;link&#34; href=&#34;https://matongxue.blog.csdn.net/article/details/109766892&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;如何理解概率论中的“矩”？&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>舒尔补</title>
        <link>https://wzwan-developer.github.io/p/202/</link>
        <pubDate>Mon, 02 Sep 2024 23:51:06 +0800</pubDate>
        
        <guid>https://wzwan-developer.github.io/p/202/</guid>
        <description>&lt;img src="https://wzwan-developer.github.io/p/202/gaussian-normal-distribution-graph.png" alt="Featured image of post 舒尔补" /&gt;&lt;h2 id=&#34;舒尔补定义&#34;&gt;舒尔补定义
&lt;/h2&gt;&lt;p&gt;给定任意的矩阵块 $M$ ，如下所示：&lt;/p&gt;
$$M=\begin{bmatrix} A &amp; B\\  C &amp;D\\ \end{bmatrix}$$&lt;ul&gt;
&lt;li&gt;如果，矩阵块 $D$ 是可逆的，则 $A − B D^{-1}  C$称之为 $D$ 关于 $M$的舒尔补。&lt;/li&gt;
&lt;li&gt;如果，矩阵块 $A$ 是可逆的，则 $D − CA^{-1}  B$称之为 $A$ 关于 $M$的舒尔补。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;舒尔补的定理推导&#34;&gt;舒尔补的定理推导
&lt;/h2&gt;&lt;p&gt;将$M$矩阵分别变成上三角或者下三角：
&lt;/p&gt;
$$\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; B\\ C &amp; D\end{bmatrix}=\begin{bmatrix}A &amp; B\\ 0 &amp; \Delta _{A}\end{bmatrix}$$$$\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}
\begin{bmatrix}I &amp; -A^{-1}B\\  0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; 0\\ C &amp; \Delta _{A}\end{bmatrix}$$&lt;p&gt;其中${\Delta _{A} =D-CA^{-1}B }$。联合起来，将${M}$变形为对角形：
&lt;/p&gt;
$$\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; B\\ C &amp; D\end{bmatrix}
\begin{bmatrix}I &amp; -A^{-1}B\\  0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; 0\\ 0 &amp; \Delta _{A}\end{bmatrix}
$$&lt;p&gt;
反过来亦可从对角矩阵恢复${M}$:
&lt;/p&gt;
$$\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; 0\\ 0&amp; \Delta _{A}\end{bmatrix}
\begin{bmatrix}I &amp; A^{-1}B\\  0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; B\\ C &amp; D\ \end{bmatrix}
$$&lt;h2 id=&#34;用途&#34;&gt;用途
&lt;/h2&gt;&lt;h3 id=&#34;快速求矩阵的逆&#34;&gt;快速求矩阵的逆
&lt;/h3&gt;&lt;p&gt;矩阵${M}$可以改写为：
&lt;/p&gt;
$$M=\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}=\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; 0\\ 0&amp; \Delta _{A}\end{bmatrix}
\begin{bmatrix}I &amp; A^{-1}B\\  0&amp;I\\\end{bmatrix}$$$$M^{-1}=\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}^{-1}=\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}^{-1}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; A^{-1}B\\  0&amp;I\\\end{bmatrix}^{-1}\\  =\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; -A^{-1}B\\  0&amp;I\\\end{bmatrix}\\=\begin{bmatrix}A^{-1}+A^{-1}B{\Delta _A}^{-1}CA^{-1}&amp;-A^{-1}B{\Delta _A}^{-1}\\-{\Delta _A}^{-1}CA^{-1}&amp;{\Delta _A}^{-1} \end{bmatrix}
$$&lt;h3 id=&#34;舒尔补在信息矩阵求解中的使用&#34;&gt;舒尔补在信息矩阵求解中的使用
&lt;/h3&gt;&lt;p&gt;协方差矩阵&lt;/p&gt;
$$\sum=\begin{bmatrix}A &amp; C^{T}\\C&amp;D\end{bmatrix}$$&lt;p&gt;,
则信息矩阵&lt;/p&gt;
$$\sum^{-1}=\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\\=\begin{bmatrix}A^{-1}+A^{-1}C^{T}{\Delta _A}^{-1}CA^{-1}&amp;-A^{-1}C^{T}{\Delta _A}^{-1}\\-{\Delta _A}^{-1}CA^{-1}&amp;{\Delta _A}^{-1} \end{bmatrix}\\\stackrel{\triangle}{=}\begin{bmatrix}\Lambda_{aa}&amp;\Lambda _{ab}\\\Lambda _{ba}&amp;\Lambda _{bb} \end{bmatrix}$$&lt;p&gt;
其中，由上式可推导得$A^{-1}=\Lambda _{aa}-\Lambda _{ab}\Lambda _{bb}^{-1}\Lambda _{ba}$, 以及$D^{-1}=\Lambda _{bb}-\Lambda _{ba}\Lambda _{aa}^{-1}\Lambda _{ab}$，它们即为下次优化会使用的先验信息矩阵（边际概率的信息矩阵）。&lt;/p&gt;
&lt;h3 id=&#34;通过舒尔补分解多元高斯分布&#34;&gt;通过舒尔补分解多元高斯分布
&lt;/h3&gt;&lt;p&gt;假设多元变量$M$服从高斯分布，且由两部分组成：&lt;/p&gt;
$$ x=\begin{bmatrix}a\\b\end{bmatrix} $$&lt;p&gt;,变量构成的协方差矩阵等于&lt;/p&gt;
$$\sum=\begin{bmatrix} A &amp; C^{T} \\C&amp;D \end{bmatrix}$$&lt;p&gt;,其中$A=cov(a,a)$,$C=cov(a,b)$,$D=cov(b,b)$。
则$x$的概率分布为：
&lt;/p&gt;
$$P(a,b)=P(a)P(b|a)\propto exp(-\frac{1}{2} \begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\begin{bmatrix}a&amp;b\end{bmatrix})$$&lt;p&gt;。使用上一节内容将矩阵转化为对角矩阵
&lt;/p&gt;
$$ \begin{align}
P(a,b) \\ 
\propto  exp\left (  -\frac{1}{2}\begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\begin{bmatrix}a&amp;b\end{bmatrix}\right) \\
  \propto exp \left( -\frac{1}{2}\begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; -A^{-1}B\\  0&amp;I\\\end{bmatrix}\begin{bmatrix}a&amp;b\end{bmatrix})\right )\\
  \propto exp\left( -\frac{1}{2}\begin{bmatrix}a^{T}&amp;(b-CA^{-1}a)^{T}\end{bmatrix}\begin{bmatrix}A^{-1}&amp;0\\0&amp;{\Delta _A^{-1}}\end{bmatrix}\begin{bmatrix}a\\b-CA^{-1}a\end{bmatrix}\right)\\
  \propto exp \left( -\frac{1}{2}(a^TA^{-1}a)+(b-CA^{-1}a)^{T} \Delta _A^{-1}(b-CA^{-1}a) \right)\\
  \propto exp \left( -\frac{1}{2}a^{T}A^{-1}a\right)exp \left ( -\frac{1}{2}(b-CA^{-1}a)^{T}\Delta _A^{-1}(b-CA^{-1}a)\right)\\
  \propto P(a)P(b|a)
  \\\end{align}$$&lt;p&gt;
在《机器人学中的状态估计》2.2.3章节&amp;quot;联合概率密度函数，分解与推断&amp;quot;可见相似内容,其实就是高斯推断，套用相关模型$P(a)$是观测（边际概率），$P(b|a)$是后验概率，$P(a|b)$是传感器模型。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>边缘化</title>
        <link>https://wzwan-developer.github.io/p/201/</link>
        <pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>https://wzwan-developer.github.io/p/201/</guid>
        <description>&lt;img src="https://wzwan-developer.github.io/p/201/gaussian-normal-distribution-graph.png" alt="Featured image of post 边缘化" /&gt;&lt;h2 id=&#34;从优化角度理解边缘化&#34;&gt;从优化角度理解边缘化
&lt;/h2&gt;&lt;h3 id=&#34;模型&#34;&gt;模型
&lt;/h3&gt;&lt;p&gt;优化问题具有如下通用形式：
$ HX=b $
并且可以拆解成如下的形式：
&lt;/p&gt;
$$\begin{bmatrix}H_{mm}&amp;H_{mr}\\H_{rm}&amp;H_{rr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}b_m\\b_r\end{bmatrix} $$&lt;p&gt;
拆解的目的是通过边缘化将$X_m$从状态量里删除掉，但是要保留它的约束。在划窗模式里，这个$X_m$为要边缘化掉的量。&lt;/p&gt;
&lt;h3 id=&#34;过程&#34;&gt;过程
&lt;/h3&gt;&lt;p&gt;对$H$矩阵利用&lt;a class=&#34;link&#34; href=&#34;https://wzwan-developer.github.io/p/202/&#34; &gt;舒尔补&lt;/a&gt; 进行三角化，如下所示：
&lt;/p&gt;
$$\begin{bmatrix}I&amp;0\\-H_{rm}H_{mm}^{-1}&amp;I\end{bmatrix}\begin{bmatrix}H_{mm}&amp;H_{mr}\\H_{rm}&amp;H_{rr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}I&amp;0\\-H_{rm}H_{mm}^{-1}&amp;I\end{bmatrix}\begin{bmatrix}b_m\\b_r\end{bmatrix}$$&lt;p&gt;
化简可得：
&lt;/p&gt;
$$\begin{bmatrix}H_{mm}&amp;H_{mr}\\0&amp;H_{rr}-H_{rm}H_{mm}^{-1}H_{mr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}b_m\\b_r-H_{rm}H_{mm}^{-1}b_m\end{bmatrix}$$&lt;p&gt;
由上式可得：
&lt;/p&gt;
$$(H_{rr}-H_{rm}H_{mm}^{-1}H_{mr})X_r=b_r-H_{rm}H_{mm}^{-1}b_{m}$$&lt;p&gt;
意义：此时可以不依赖$X_m$求解出$X_r$,若我们只关心$X_r$的值，则可以把$X_m$从模型中删除。&lt;/p&gt;
&lt;h2 id=&#34;从滤波角度理解边缘化&#34;&gt;从滤波角度理解边缘化
&lt;/h2&gt;&lt;h3 id=&#34;模型-1&#34;&gt;模型
&lt;/h3&gt;&lt;p&gt;运动模型和观测模型定义（可见《机器人学中的状态估计》3.3.1 ”问题定义“章节）
如下：
&lt;/p&gt;
$$\begin{align}
x_k=A_{k-1}x_{k-1}+v_k+w_k,&amp;k=1...K\\
y_k=C_kx_k+n_k,&amp;k=0...K
\end{align}$$&lt;h3 id=&#34;map估计角度&#34;&gt;MAP估计角度
&lt;/h3&gt;&lt;p&gt;优化目标函数定义（详情参考《机器人学中的状态估计》3.1.2 &amp;ldquo;最大后验估计&amp;rdquo; 章节）如下：
&lt;/p&gt;
$$\hat{x}=arg\underset{x}{min} J(x)$$&lt;p&gt;
其中$J(x)=\sum_{k=0}^{K}(J_{v,k}(x)+J_{y,k}(x)$,$J_{v,k}(x))$见式(3.9.a),$J_{y,k}(x)$见式(3.9.b)。
再次对形式进行以下提升，将所有时刻的状态整理为向量x,并把所有时刻已知数据整理为z。对问题进行一定的简化，可得
$J(x)=\frac{1}{2}\left(z-Hx \right)^{T}W^{-1}(z-Hx)$（式3.14）
对其进行求解最小值，可求解它的一阶导数，并使一阶导为0；
&lt;/p&gt;
$${\frac{\partial J(x)}{\partial x^{T}}}|_x=-H^{T}W^{-1}(z-H\hat{x})=0 \Rightarrow (H^{T}W^{-1}H)\hat{x}=H^{T}W^{-1}z$$&lt;p&gt;
&lt;em&gt;注：此时形式以及是接近优化角度的$HX=b$。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;滤波角度&#34;&gt;滤波角度
&lt;/h3&gt;&lt;p&gt;由于马尔可夫性，当前时刻仅与前一时刻有关，由此再次参考 3.3.2章节 &amp;ldquo;通过MAP推导卡尔曼滤波&amp;rdquo;，
假设已经有了k-1时刻的后验估计&lt;/p&gt;
$$ \{ \hat {x}_{k-1} ,\hat{P}_{k-1}\}$$&lt;p&gt;,目标是计算&lt;/p&gt;
$$ \{ \hat {x}_{k} ,\hat{P}_{k}\}$$&lt;p&gt;，我们使用k-1时刻的后验估计加上k时刻的$v_k$,$y_k$来估计&lt;/p&gt;
$$ \{ \hat {x}_{k} ,\hat{P}_{k}\}$$&lt;p&gt;。
为了推导该过程，定义
&lt;/p&gt;
$$ z=\begin{bmatrix}\hat{x}_{k-1}\\v_k\\y_k\end{bmatrix},H=\begin{bmatrix}I&amp;&amp;\\-A_{k-1}&amp;I\\&amp;&amp;C_{k}\end{bmatrix},W=\begin{bmatrix}\hat {P}_{k-1}&amp;&amp;\\&amp;Q_k&amp;\\&amp;&amp;R_k\end{bmatrix}$$&lt;p&gt;
则模型的解为&lt;/p&gt;
$$(H_{k}^{T}W_{k}^{-1}H_{k})\hat{x}=H_{k}^{T}W_{k}^{-1}z_k$$&lt;p&gt;，
其中&lt;/p&gt;
$$\hat{x}=\begin{bmatrix}\hat{x&#39;}_{k-1}\\\hat{x}_k\end{bmatrix}$$&lt;p&gt;
&lt;img src=&#34;https://wzwan-developer.github.io/p/201/image00.png&#34;
	width=&#34;714&#34;
	height=&#34;352&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/201/image00_hu2802948725242621933.png 480w, https://wzwan-developer.github.io/p/201/image00_hu16496475951586850660.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;202&#34;
		data-flex-basis=&#34;486px&#34;
	
&gt;
借助本文第一节，目标为从$x$变量中删除$\hat{x&#39;}_{k-1}$，执行舒尔补可得
&lt;img src=&#34;https://wzwan-developer.github.io/p/201/image01.png&#34;
	width=&#34;707&#34;
	height=&#34;463&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/201/image01_hu12842677403844977782.png 480w, https://wzwan-developer.github.io/p/201/image01_hu10364658897043807725.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;366px&#34;
	
&gt;
&lt;img src=&#34;https://wzwan-developer.github.io/p/201/image02.png&#34;
	width=&#34;1371&#34;
	height=&#34;694&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/201/image02_hu7680983322299133859.png 480w, https://wzwan-developer.github.io/p/201/image02_hu10029282904931644770.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;474px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>归档</title>
        <link>https://wzwan-developer.github.io/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://wzwan-developer.github.io/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>搜索</title>
        <link>https://wzwan-developer.github.io/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://wzwan-developer.github.io/search/</guid>
        <description></description>
        </item>
        <item>
        <title>友情链接</title>
        <link>https://wzwan-developer.github.io/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://wzwan-developer.github.io/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
