[{"content":"概率论中的“矩” 彩票的问题 假设福利彩票，每一注两元钱，且中奖的概率分布如下： 其中，概率的“称”如下所示： 此时我们称量一下中奖500万元： 上述结果表明：不确定的500万元等价于确定的0.5元。此时将所有的中奖概率刻画上去： 上述结果等于\n$$1.5 = 5\\times 10\\% + 100\\times0.5\\% + 5000000\\times0.000001\\% $$ 结果表明一张彩票成本两元，但是期望获得的收益为1.5元，每买一张都会亏损0.5元。\n“矩” 一阶矩 上述我们计算的就是概率的一阶矩，也就是期望（expectation/mean）。 $$ E[X]=\\sum p_{i}x_{i} $$含义如下： 二阶矩 二阶矩是广为认知的协方差矩阵$\\Sigma$ $$ \\Sigma=E[(X-\\mu)^{2}]=\\sum_{i}p_{i}(x_{i}-\\mu)^{2} $$高阶矩 三阶矩称为偏度，四阶矩称为峰度。各有用途但是共同的特点为称量之后才能使用。\n参考链接 [1]如何理解概率论中的“矩”？\n","date":"2024-09-09T11:12:48+08:00","image":"https://wzwan-developer.github.io/p/203/gaussian-normal-distribution-graph_hu15871502155684369944.png","permalink":"https://wzwan-developer.github.io/p/203/","title":"如何理解概率论中的“矩”"},{"content":"舒尔补定义 给定任意的矩阵块 $M$ ，如下所示：\n$$M=\\begin{bmatrix} A \u0026 B\\\\ C \u0026D\\\\ \\end{bmatrix}$$ 如果，矩阵块 $D$ 是可逆的，则 $A − B D^{-1} C$称之为 $D$ 关于 $M$的舒尔补。 如果，矩阵块 $A$ 是可逆的，则 $D − CA^{-1} B$称之为 $A$ 关于 $M$的舒尔补。 舒尔补的定理推导 将$M$矩阵分别变成上三角或者下三角： $$\\begin{bmatrix}I \u0026 0\\\\ -CA^{-1} \u0026I\\\\\\end{bmatrix} \\begin{bmatrix}A \u0026 B\\\\ C \u0026 D\\end{bmatrix}=\\begin{bmatrix}A \u0026 B\\\\ 0 \u0026 \\Delta _{A}\\end{bmatrix}$$$$\\begin{bmatrix}A \u0026 B\\\\ C\u0026D\\end{bmatrix} \\begin{bmatrix}I \u0026 -A^{-1}B\\\\ 0\u0026I\\\\\\end{bmatrix}= \\begin{bmatrix}A \u0026 0\\\\ C \u0026 \\Delta _{A}\\end{bmatrix}$$其中${\\Delta _{A} =D-CA^{-1}B }$。联合起来，将${M}$变形为对角形： $$\\begin{bmatrix}I \u0026 0\\\\ -CA^{-1} \u0026I\\\\\\end{bmatrix} \\begin{bmatrix}A \u0026 B\\\\ C \u0026 D\\end{bmatrix} \\begin{bmatrix}I \u0026 -A^{-1}B\\\\ 0\u0026I\\\\\\end{bmatrix}= \\begin{bmatrix}A \u0026 0\\\\ 0 \u0026 \\Delta _{A}\\end{bmatrix} $$ 反过来亦可从对角矩阵恢复${M}$: $$\\begin{bmatrix}I \u0026 0\\\\ CA^{-1} \u0026I\\\\\\end{bmatrix} \\begin{bmatrix}A \u0026 0\\\\ 0\u0026 \\Delta _{A}\\end{bmatrix} \\begin{bmatrix}I \u0026 A^{-1}B\\\\ 0\u0026I\\\\\\end{bmatrix}= \\begin{bmatrix}A \u0026 B\\\\ C \u0026 D\\ \\end{bmatrix} $$用途 快速求矩阵的逆 矩阵${M}$可以改写为： $$M=\\begin{bmatrix}A \u0026 B\\\\ C\u0026D\\end{bmatrix}=\\begin{bmatrix}I \u0026 0\\\\ CA^{-1} \u0026I\\\\\\end{bmatrix} \\begin{bmatrix}A \u0026 0\\\\ 0\u0026 \\Delta _{A}\\end{bmatrix} \\begin{bmatrix}I \u0026 A^{-1}B\\\\ 0\u0026I\\\\\\end{bmatrix}$$$$M^{-1}=\\begin{bmatrix}A \u0026 B\\\\ C\u0026D\\end{bmatrix}^{-1}=\\begin{bmatrix}I \u0026 0\\\\ CA^{-1} \u0026I\\\\\\end{bmatrix}^{-1}\\begin{bmatrix}A^{-1} \u0026 0\\\\ 0\u0026 \\Delta _{A}^{-1}\\end{bmatrix}\\begin{bmatrix}I \u0026 A^{-1}B\\\\ 0\u0026I\\\\\\end{bmatrix}^{-1}\\\\ =\\begin{bmatrix}I \u0026 0\\\\ -CA^{-1} \u0026I\\\\\\end{bmatrix}\\begin{bmatrix}A^{-1} \u0026 0\\\\ 0\u0026 \\Delta _{A}^{-1}\\end{bmatrix}\\begin{bmatrix}I \u0026 -A^{-1}B\\\\ 0\u0026I\\\\\\end{bmatrix}\\\\=\\begin{bmatrix}A^{-1}+A^{-1}B{\\Delta _A}^{-1}CA^{-1}\u0026-A^{-1}B{\\Delta _A}^{-1}\\\\-{\\Delta _A}^{-1}CA^{-1}\u0026{\\Delta _A}^{-1} \\end{bmatrix} $$舒尔补在信息矩阵求解中的使用 协方差矩阵\n$$\\sum=\\begin{bmatrix}A \u0026 C^{T}\\\\C\u0026D\\end{bmatrix}$$, 则信息矩阵\n$$\\sum^{-1}=\\begin{bmatrix}A\u0026C^{T}\\\\C\u0026D\\end{bmatrix}^{-1}\\\\=\\begin{bmatrix}A^{-1}+A^{-1}C^{T}{\\Delta _A}^{-1}CA^{-1}\u0026-A^{-1}C^{T}{\\Delta _A}^{-1}\\\\-{\\Delta _A}^{-1}CA^{-1}\u0026{\\Delta _A}^{-1} \\end{bmatrix}\\\\\\stackrel{\\triangle}{=}\\begin{bmatrix}\\Lambda_{aa}\u0026\\Lambda _{ab}\\\\\\Lambda _{ba}\u0026\\Lambda _{bb} \\end{bmatrix}$$ 其中，由上式可推导得$A^{-1}=\\Lambda _{aa}-\\Lambda _{ab}\\Lambda _{bb}^{-1}\\Lambda _{ba}$, 以及$D^{-1}=\\Lambda _{bb}-\\Lambda _{ba}\\Lambda _{aa}^{-1}\\Lambda _{ab}$，它们即为下次优化会使用的先验信息矩阵（边际概率的信息矩阵）。\n通过舒尔补分解多元高斯分布 假设多元变量$M$服从高斯分布，且由两部分组成：\n$$ x=\\begin{bmatrix}a\\\\b\\end{bmatrix} $$,变量构成的协方差矩阵等于\n$$\\sum=\\begin{bmatrix} A \u0026 C^{T} \\\\C\u0026D \\end{bmatrix}$$,其中$A=cov(a,a)$,$C=cov(a,b)$,$D=cov(b,b)$。 则$x$的概率分布为： $$P(a,b)=P(a)P(b|a)\\propto exp(-\\frac{1}{2} \\begin{bmatrix}a\\\\b\\end{bmatrix}^{T}\\begin{bmatrix}A\u0026C^{T}\\\\C\u0026D\\end{bmatrix}^{-1}\\begin{bmatrix}a\u0026b\\end{bmatrix})$$。使用上一节内容将矩阵转化为对角矩阵 $$ \\begin{align} P(a,b) \\\\ \\propto exp\\left ( -\\frac{1}{2}\\begin{bmatrix}a\\\\b\\end{bmatrix}^{T}\\begin{bmatrix}A\u0026C^{T}\\\\C\u0026D\\end{bmatrix}^{-1}\\begin{bmatrix}a\u0026b\\end{bmatrix}\\right) \\\\ \\propto exp \\left( -\\frac{1}{2}\\begin{bmatrix}a\\\\b\\end{bmatrix}^{T}\\begin{bmatrix}I \u0026 0\\\\ -CA^{-1} \u0026I\\\\\\end{bmatrix}\\begin{bmatrix}A^{-1} \u0026 0\\\\ 0\u0026 \\Delta _{A}^{-1}\\end{bmatrix}\\begin{bmatrix}I \u0026 -A^{-1}B\\\\ 0\u0026I\\\\\\end{bmatrix}\\begin{bmatrix}a\u0026b\\end{bmatrix})\\right )\\\\ \\propto exp\\left( -\\frac{1}{2}\\begin{bmatrix}a^{T}\u0026(b-CA^{-1}a)^{T}\\end{bmatrix}\\begin{bmatrix}A^{-1}\u00260\\\\0\u0026{\\Delta _A^{-1}}\\end{bmatrix}\\begin{bmatrix}a\\\\b-CA^{-1}a\\end{bmatrix}\\right)\\\\ \\propto exp \\left( -\\frac{1}{2}(a^TA^{-1}a)+(b-CA^{-1}a)^{T} \\Delta _A^{-1}(b-CA^{-1}a) \\right)\\\\ \\propto exp \\left( -\\frac{1}{2}a^{T}A^{-1}a\\right)exp \\left ( -\\frac{1}{2}(b-CA^{-1}a)^{T}\\Delta _A^{-1}(b-CA^{-1}a)\\right)\\\\ \\propto P(a)P(b|a) \\\\\\end{align}$$ 在《机器人学中的状态估计》2.2.3章节\u0026quot;联合概率密度函数，分解与推断\u0026quot;可见相似内容,其实就是高斯推断，套用相关模型$P(a)$是观测（边际概率），$P(b|a)$是后验概率，$P(a|b)$是传感器模型。\n","date":"2024-09-02T23:51:06+08:00","image":"https://wzwan-developer.github.io/p/202/gaussian-normal-distribution-graph_hu15871502155684369944.png","permalink":"https://wzwan-developer.github.io/p/202/","title":"舒尔补"},{"content":"从优化角度理解边缘化 模型 优化问题具有如下通用形式： $ HX=b $ 并且可以拆解成如下的形式： $$\\begin{bmatrix}H_{mm}\u0026H_{mr}\\\\H_{rm}\u0026H_{rr}\\end{bmatrix}\\begin{bmatrix}X_m\\\\X_r\\end{bmatrix}=\\begin{bmatrix}b_m\\\\b_r\\end{bmatrix} $$ 拆解的目的是通过边缘化将$X_m$从状态量里删除掉，但是要保留它的约束。在划窗模式里，这个$X_m$为要边缘化掉的量。\n过程 对$H$矩阵利用舒尔补 进行三角化，如下所示： $$\\begin{bmatrix}I\u00260\\\\-H_{rm}H_{mm}^{-1}\u0026I\\end{bmatrix}\\begin{bmatrix}H_{mm}\u0026H_{mr}\\\\H_{rm}\u0026H_{rr}\\end{bmatrix}\\begin{bmatrix}X_m\\\\X_r\\end{bmatrix}=\\begin{bmatrix}I\u00260\\\\-H_{rm}H_{mm}^{-1}\u0026I\\end{bmatrix}\\begin{bmatrix}b_m\\\\b_r\\end{bmatrix}$$ 化简可得： $$\\begin{bmatrix}H_{mm}\u0026H_{mr}\\\\0\u0026H_{rr}-H_{rm}H_{mm}^{-1}H_{mr}\\end{bmatrix}\\begin{bmatrix}X_m\\\\X_r\\end{bmatrix}=\\begin{bmatrix}b_m\\\\b_r-H_{rm}H_{mm}^{-1}b_m\\end{bmatrix}$$ 由上式可得： $$(H_{rr}-H_{rm}H_{mm}^{-1}H_{mr})X_r=b_r-H_{rm}H_{mm}^{-1}b_{m}$$ 意义：此时可以不依赖$X_m$求解出$X_r$,若我们只关心$X_r$的值，则可以把$X_m$从模型中删除。\n从滤波角度理解边缘化 模型 运动模型和观测模型定义（可见《机器人学中的状态估计》3.3.1 ”问题定义“章节） 如下： $$\\begin{align} x_k=A_{k-1}x_{k-1}+v_k+w_k,\u0026k=1...K\\\\ y_k=C_kx_k+n_k,\u0026k=0...K \\end{align}$$MAP估计角度 优化目标函数定义（详情参考《机器人学中的状态估计》3.1.2 \u0026ldquo;最大后验估计\u0026rdquo; 章节）如下： $$\\hat{x}=arg\\underset{x}{min} J(x)$$ 其中$J(x)=\\sum_{k=0}^{K}(J_{v,k}(x)+J_{y,k}(x)$,$J_{v,k}(x))$见式(3.9.a),$J_{y,k}(x)$见式(3.9.b)。 再次对形式进行以下提升，将所有时刻的状态整理为向量x,并把所有时刻已知数据整理为z。对问题进行一定的简化，可得 $J(x)=\\frac{1}{2}\\left(z-Hx \\right)^{T}W^{-1}(z-Hx)$（式3.14） 对其进行求解最小值，可求解它的一阶导数，并使一阶导为0； $${\\frac{\\partial J(x)}{\\partial x^{T}}}|_x=-H^{T}W^{-1}(z-H\\hat{x})=0 \\Rightarrow (H^{T}W^{-1}H)\\hat{x}=H^{T}W^{-1}z$$ 注：此时形式以及是接近优化角度的$HX=b$。\n滤波角度 由于马尔可夫性，当前时刻仅与前一时刻有关，由此再次参考 3.3.2章节 \u0026ldquo;通过MAP推导卡尔曼滤波\u0026rdquo;， 假设已经有了k-1时刻的后验估计\n$$ \\{ \\hat {x}_{k-1} ,\\hat{P}_{k-1}\\}$$,目标是计算\n$$ \\{ \\hat {x}_{k} ,\\hat{P}_{k}\\}$$，我们使用k-1时刻的后验估计加上k时刻的$v_k$,$y_k$来估计\n$$ \\{ \\hat {x}_{k} ,\\hat{P}_{k}\\}$$。 为了推导该过程，定义 $$ z=\\begin{bmatrix}\\hat{x}_{k-1}\\\\v_k\\\\y_k\\end{bmatrix},H=\\begin{bmatrix}I\u0026\u0026\\\\-A_{k-1}\u0026I\\\\\u0026\u0026C_{k}\\end{bmatrix},W=\\begin{bmatrix}\\hat {P}_{k-1}\u0026\u0026\\\\\u0026Q_k\u0026\\\\\u0026\u0026R_k\\end{bmatrix}$$ 则模型的解为\n$$(H_{k}^{T}W_{k}^{-1}H_{k})\\hat{x}=H_{k}^{T}W_{k}^{-1}z_k$$， 其中\n$$\\hat{x}=\\begin{bmatrix}\\hat{x'}_{k-1}\\\\\\hat{x}_k\\end{bmatrix}$$ 借助本文第一节，目标为从$x$变量中删除$\\hat{x\u0026rsquo;}_{k-1}$，执行舒尔补可得 ","date":"2024-09-02T00:00:00Z","image":"https://wzwan-developer.github.io/p/201/gaussian-normal-distribution-graph_hu15871502155684369944.png","permalink":"https://wzwan-developer.github.io/p/201/","title":"边缘化"}]