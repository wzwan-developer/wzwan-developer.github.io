<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>数学理论 on wzwan</title>
        <link>https://wzwan-developer.github.io/categories/math/</link>
        <description>Recent content in 数学理论 on wzwan</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>wzwan</copyright><atom:link href="https://wzwan-developer.github.io/categories/math/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>如何理解概率论中的“矩”</title>
        <link>https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/</link>
        <pubDate>Mon, 09 Sep 2024 11:12:48 +0800</pubDate>
        
        <guid>https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/</guid>
        <description>&lt;img src="https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/gaussian-normal-distribution-graph.png" alt="Featured image of post 如何理解概率论中的“矩”" /&gt;&lt;h2 id=&#34;概率论中的矩&#34;&gt;概率论中的“矩”
&lt;/h2&gt;&lt;h3 id=&#34;彩票的问题&#34;&gt;彩票的问题
&lt;/h3&gt;&lt;p&gt;假设福利彩票，每一注两元钱，且中奖的概率分布如下：
&lt;img src=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph1.png&#34;
	width=&#34;359&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph1_hu7074458194076008173.png 480w, https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph1_hu11398514835930632876.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;概率分布图&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;89&#34;
		data-flex-basis=&#34;214px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;其中，概率的“称”如下所示：
&lt;img src=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph2.png&#34;
	width=&#34;613&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph2_hu1581581583894811490.png 480w, https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph2_hu3775081096410037377.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;概率的称&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;此时我们称量一下中奖500万元：
&lt;img src=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph3.png&#34;
	width=&#34;613&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph3_hu9993873231620659325.png 480w, https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph3_hu8003149627366197390.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;500万元&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;上述结果表明：不确定的500万元等价于确定的0.5元。此时将所有的中奖概率刻画上去：
&lt;img src=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph4.png&#34;
	width=&#34;613&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph4_hu8029704338762354472.png 480w, https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph4_hu18134410058688724433.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;365px&#34;
	
&gt;&lt;/p&gt;
$$1.5 = 5\times 10\% + 100\times0.5\% + 5000000\times0.000001\% $$&lt;p&gt;
结果表明一张彩票成本两元，但是期望获得的收益为1.5元，每买一张都会亏损0.5元。&lt;/p&gt;
&lt;h2 id=&#34;矩&#34;&gt;“矩”
&lt;/h2&gt;&lt;h3 id=&#34;一阶矩&#34;&gt;一阶矩
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;上述我们计算的就是概率的一阶矩，也就是期望（expectation/mean）。
&lt;/code&gt;&lt;/pre&gt;
$$
E[X]=\sum p_{i}x_{i}
$$&lt;p&gt;含义如下：
&lt;img src=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph5.png&#34;
	width=&#34;572&#34;
	height=&#34;402&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph5_hu18168430126999916666.png 480w, https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_03/graph5_hu8960376600484578208.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;”期望含义“&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;341px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;二阶矩&#34;&gt;二阶矩
&lt;/h3&gt;$$
    \Sigma=E[(X-\mu)^{2}]=\sum_{i}p_{i}(x_{i}-\mu)^{2}
$$&lt;h3 id=&#34;高阶矩&#34;&gt;高阶矩
&lt;/h3&gt;&lt;p&gt;三阶矩称为偏度，四阶矩称为峰度。各有用途但是共同的特点为称量之后才能使用。&lt;/p&gt;
&lt;h2 id=&#34;参考链接&#34;&gt;参考链接
&lt;/h2&gt;&lt;p&gt;[1]&lt;a class=&#34;link&#34; href=&#34;https://matongxue.blog.csdn.net/article/details/109766892&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;如何理解概率论中的“矩”？&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>舒尔补</title>
        <link>https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_02/</link>
        <pubDate>Mon, 02 Sep 2024 23:51:06 +0800</pubDate>
        
        <guid>https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_02/</guid>
        <description>&lt;img src="https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_02/gaussian-normal-distribution-graph.png" alt="Featured image of post 舒尔补" /&gt;&lt;h2 id=&#34;舒尔补定义&#34;&gt;舒尔补定义
&lt;/h2&gt;&lt;p&gt;给定任意的矩阵块 $M$ ，如下所示：&lt;/p&gt;
$$M=\begin{bmatrix} A &amp; B\\  C &amp;D\\ \end{bmatrix}$$&lt;ul&gt;
&lt;li&gt;如果，矩阵块 $D$ 是可逆的，则 $A − B D^{-1}  C$称之为 $D$ 关于 $M$的舒尔补。&lt;/li&gt;
&lt;li&gt;如果，矩阵块 $A$ 是可逆的，则 $D − CA^{-1}  B$称之为 $A$ 关于 $M$的舒尔补。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;舒尔补的定理推导&#34;&gt;舒尔补的定理推导
&lt;/h2&gt;$$\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; B\\ C &amp; D\end{bmatrix}=\begin{bmatrix}A &amp; B\\ 0 &amp; \Delta _{A}\end{bmatrix}$$$$\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}
\begin{bmatrix}I &amp; -A^{-1}B\\  0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; 0\\ C &amp; \Delta _{A}\end{bmatrix}$$$$\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; B\\ C &amp; D\end{bmatrix}
\begin{bmatrix}I &amp; -A^{-1}B\\  0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; 0\\ 0 &amp; \Delta _{A}\end{bmatrix}
$$$$\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; 0\\ 0&amp; \Delta _{A}\end{bmatrix}
\begin{bmatrix}I &amp; A^{-1}B\\  0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; B\\ C &amp; D\ \end{bmatrix}
$$&lt;h2 id=&#34;用途&#34;&gt;用途
&lt;/h2&gt;&lt;h3 id=&#34;快速求矩阵的逆&#34;&gt;快速求矩阵的逆
&lt;/h3&gt;$$M=\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}=\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; 0\\ 0&amp; \Delta _{A}\end{bmatrix}
\begin{bmatrix}I &amp; A^{-1}B\\  0&amp;I\\\end{bmatrix}$$$$M^{-1}=\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}^{-1}=\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}^{-1}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; A^{-1}B\\  0&amp;I\\\end{bmatrix}^{-1}\\  =\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; -A^{-1}B\\  0&amp;I\\\end{bmatrix}\\=\begin{bmatrix}A^{-1}+A^{-1}B{\Delta _A}^{-1}CA^{-1}&amp;-A^{-1}B{\Delta _A}^{-1}\\-{\Delta _A}^{-1}CA^{-1}&amp;{\Delta _A}^{-1} \end{bmatrix}
$$&lt;h3 id=&#34;舒尔补在信息矩阵求解中的使用&#34;&gt;舒尔补在信息矩阵求解中的使用
&lt;/h3&gt;$$\sum=\begin{bmatrix}A &amp; C^{T}\\C&amp;D\end{bmatrix}$$$$\sum^{-1}=\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\\=\begin{bmatrix}A^{-1}+A^{-1}C^{T}{\Delta _A}^{-1}CA^{-1}&amp;-A^{-1}C^{T}{\Delta _A}^{-1}\\-{\Delta _A}^{-1}CA^{-1}&amp;{\Delta _A}^{-1} \end{bmatrix}\\\stackrel{\triangle}{=}\begin{bmatrix}\Lambda_{aa}&amp;\Lambda _{ab}\\\Lambda _{ba}&amp;\Lambda _{bb} \end{bmatrix}$$&lt;p&gt;
其中，由上式可推导得$A^{-1}=\Lambda _{aa}-\Lambda _{ab}\Lambda _{bb}^{-1}\Lambda _{ba}$, 以及$D^{-1}=\Lambda _{bb}-\Lambda _{ba}\Lambda _{aa}^{-1}\Lambda _{ab}$，它们即为下次优化会使用的先验信息矩阵（边际概率的信息矩阵）。&lt;/p&gt;
&lt;h3 id=&#34;通过舒尔补分解多元高斯分布&#34;&gt;通过舒尔补分解多元高斯分布
&lt;/h3&gt;$$ x=\begin{bmatrix}a\\b\end{bmatrix} $$$$\sum=\begin{bmatrix} A &amp; C^{T} \\C&amp;D \end{bmatrix}$$$$P(a,b)=P(a)P(b|a)\propto exp(-\frac{1}{2} \begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\begin{bmatrix}a&amp;b\end{bmatrix})$$$$ \begin{align}
P(a,b) \\ 
\propto  exp\left (  -\frac{1}{2}\begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\begin{bmatrix}a&amp;b\end{bmatrix}\right) \\
  \propto exp \left( -\frac{1}{2}\begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; -A^{-1}B\\  0&amp;I\\\end{bmatrix}\begin{bmatrix}a&amp;b\end{bmatrix})\right )\\
  \propto exp\left( -\frac{1}{2}\begin{bmatrix}a^{T}&amp;(b-CA^{-1}a)^{T}\end{bmatrix}\begin{bmatrix}A^{-1}&amp;0\\0&amp;{\Delta _A^{-1}}\end{bmatrix}\begin{bmatrix}a\\b-CA^{-1}a\end{bmatrix}\right)\\
  \propto exp \left( -\frac{1}{2}(a^TA^{-1}a)+(b-CA^{-1}a)^{T} \Delta _A^{-1}(b-CA^{-1}a) \right)\\
  \propto exp \left( -\frac{1}{2}a^{T}A^{-1}a\right)exp \left ( -\frac{1}{2}(b-CA^{-1}a)^{T}\Delta _A^{-1}(b-CA^{-1}a)\right)\\
  \propto P(a)P(b|a)
  \\\end{align}$$&lt;p&gt;
在《机器人学中的状态估计》2.2.3章节&amp;quot;联合概率密度函数，分解与推断&amp;quot;可见相似内容,其实就是高斯推断，套用相关模型$P(a)$是观测（边际概率），$P(b|a)$是后验概率，$P(a|b)$是传感器模型。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>边缘化</title>
        <link>https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/</link>
        <pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate>
        
        <guid>https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/</guid>
        <description>&lt;img src="https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/gaussian-normal-distribution-graph.png" alt="Featured image of post 边缘化" /&gt;&lt;h2 id=&#34;从优化角度理解边缘化&#34;&gt;从优化角度理解边缘化
&lt;/h2&gt;&lt;h3 id=&#34;模型&#34;&gt;模型
&lt;/h3&gt;$$\begin{bmatrix}H_{mm}&amp;H_{mr}\\H_{rm}&amp;H_{rr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}b_m\\b_r\end{bmatrix} $$&lt;p&gt;
拆解的目的是通过边缘化将$X_m$从状态量里删除掉，但是要保留它的约束。在划窗模式里，这个$X_m$为要边缘化掉的量。&lt;/p&gt;
&lt;h3 id=&#34;过程&#34;&gt;过程
&lt;/h3&gt;$$\begin{bmatrix}I&amp;0\\-H_{rm}H_{mm}^{-1}&amp;I\end{bmatrix}\begin{bmatrix}H_{mm}&amp;H_{mr}\\H_{rm}&amp;H_{rr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}I&amp;0\\-H_{rm}H_{mm}^{-1}&amp;I\end{bmatrix}\begin{bmatrix}b_m\\b_r\end{bmatrix}$$$$\begin{bmatrix}H_{mm}&amp;H_{mr}\\0&amp;H_{rr}-H_{rm}H_{mm}^{-1}H_{mr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}b_m\\b_r-H_{rm}H_{mm}^{-1}b_m\end{bmatrix}$$$$(H_{rr}-H_{rm}H_{mm}^{-1}H_{mr})X_r=b_r-H_{rm}H_{mm}^{-1}b_{m}$$&lt;p&gt;
意义：此时可以不依赖$X_m$求解出$X_r$,若我们只关心$X_r$的值，则可以把$X_m$从模型中删除。&lt;/p&gt;
&lt;h2 id=&#34;从滤波角度理解边缘化&#34;&gt;从滤波角度理解边缘化
&lt;/h2&gt;&lt;h3 id=&#34;模型-1&#34;&gt;模型
&lt;/h3&gt;$$\begin{align}
x_k=A_{k-1}x_{k-1}+v_k+w_k,&amp;k=1...K\\
y_k=C_kx_k+n_k,&amp;k=0...K
\end{align}$$&lt;h3 id=&#34;map估计角度&#34;&gt;MAP估计角度
&lt;/h3&gt;$$\hat{x}=arg\underset{x}{min} J(x)$$$${\frac{\partial J(x)}{\partial x^{T}}}|_x=-H^{T}W^{-1}(z-H\hat{x})=0 \Rightarrow (H^{T}W^{-1}H)\hat{x}=H^{T}W^{-1}z$$&lt;p&gt;
&lt;em&gt;注：此时形式以及是接近优化角度的$HX=b$。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;滤波角度&#34;&gt;滤波角度
&lt;/h3&gt;$$ \{ \hat {x}_{k-1} ,\hat{P}_{k-1}\}$$$$ \{ \hat {x}_{k} ,\hat{P}_{k}\}$$$$ \{ \hat {x}_{k} ,\hat{P}_{k}\}$$$$ z=\begin{bmatrix}\hat{x}_{k-1}\\v_k\\y_k\end{bmatrix},H=\begin{bmatrix}I&amp;&amp;\\-A_{k-1}&amp;I\\&amp;&amp;C_{k}\end{bmatrix},W=\begin{bmatrix}\hat {P}_{k-1}&amp;&amp;\\&amp;Q_k&amp;\\&amp;&amp;R_k\end{bmatrix}$$$$(H_{k}^{T}W_{k}^{-1}H_{k})\hat{x}=H_{k}^{T}W_{k}^{-1}z_k$$$$\hat{x}=\begin{bmatrix}\hat{x&#39;}_{k-1}\\\hat{x}_k\end{bmatrix}$$&lt;p&gt;
&lt;img src=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image00.png&#34;
	width=&#34;714&#34;
	height=&#34;352&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image00_hu2802948725242621933.png 480w, https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image00_hu16496475951586850660.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;202&#34;
		data-flex-basis=&#34;486px&#34;
	
&gt;
借助本文第一节，目标为从$x$变量中删除$\hat{x&#39;}_{k-1}$，执行舒尔补可得
&lt;img src=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image01.png&#34;
	width=&#34;707&#34;
	height=&#34;463&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image01_hu12842677403844977782.png 480w, https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image01_hu10364658897043807725.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;152&#34;
		data-flex-basis=&#34;366px&#34;
	
&gt;
&lt;img src=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image02.png&#34;
	width=&#34;1371&#34;
	height=&#34;694&#34;
	srcset=&#34;https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image02_hu7680983322299133859.png 480w, https://wzwan-developer.github.io/p/state_estimation_for_robotics/2_01/image02_hu10029282904931644770.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;474px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
