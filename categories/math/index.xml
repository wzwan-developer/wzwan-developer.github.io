<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>数学理论 on wzwan</title><link>/categories/math/</link><description>Recent content in 数学理论 on wzwan</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>wzwan</copyright><atom:link href="/categories/math/index.xml" rel="self" type="application/rss+xml"/><item><title>Least-Squares Estimation of Transformation Parameters Between Two Point Patterns</title><link>/p/least-squares-estimation-of-transformation-parameters-between-two-point-patterns/</link><pubDate>Sun, 12 Jan 2025 14:48:16 +0800</pubDate><guid>/p/least-squares-estimation-of-transformation-parameters-between-two-point-patterns/</guid><description>&lt;img src="/p/least-squares-estimation-of-transformation-parameters-between-two-point-patterns/title.png" alt="Featured image of post Least-Squares Estimation of Transformation Parameters Between Two Point Patterns" />&lt;h2 id="文章内容">文章内容
&lt;/h2>&lt;h3 id="摘要">摘要
&lt;/h3>&lt;blockquote>
&lt;p>      &lt;em>Abstract&lt;/em>— In many applications of computer vision, the following probler is encountered. Two point patterns （sets of points）$\left\{a_i \right\} $ and $\left\{y_i\right\}$;i = 1,2 $\cdots$,n are given in m-dimensional space, and we want to find the similarity transformation parameters （rotation， translation, and scaling） that give the least mean squared error between these point patterns. Recently Arun &lt;em>et al.&lt;/em> and Horn &lt;em>et al.&lt;/em> have presented a solution of this problem. Their solution, however, sometimes fails to give a correct rotation matrix and gives a reflection instead when the data is severely corrupted. The theorem given in this correspondence is a strict solution of the problem, and it always gives the correct transformation parameters even when the data is corrupted.&lt;/p>
&lt;/blockquote>
&lt;p>      摘要——在计算机视觉的许多应用中，存在以下问题。给定两个点集合$\left\{a_i \right\} $和$\left\{y_i\right\}$； i = 1,2$\cdots$，n 在 m 维空间中，我们想找到使这两个点集合之间的最小均方误差最小的相似变换参数（旋转，平移和缩放）。最近，Aru等人和Horn等人提出了一个解决这个问题的解决方案。然而，当数据严重损坏时，他们的解决方案有时会失败，无法给出正确的旋转矩阵，反而给出一个反射矩阵。这篇文章给出的定理是该问题的一个严格解，无论数据是否损坏，它总是给出正确的变换参数。&lt;/p>
&lt;blockquote>
&lt;p>      &lt;em>Index Terms&lt;/em>—— Absolute orientation problem, computer vision, least-squares, motion estimation, singular value decomposition.&lt;/p>
&lt;/blockquote>
&lt;p>      索引术语——绝对定向问题，计算机视觉，最小二乘法，运动估计，奇异值分解。&lt;/p>
&lt;h3 id="引言">引言
&lt;/h3>&lt;blockquote>
$$e^2(R,\boldsymbol{t},c)=\frac{1}{n}\sum_{i=1}^n\left\|\boldsymbol{y}_i-(cR\boldsymbol{x}_i+\boldsymbol{t})\right\|^2\tag{1}$$&lt;p>The dimensionality $m$ is usually 2 or 3.&lt;/p>
&lt;/blockquote>
$$e^2(R,\boldsymbol{t},c)=\frac{1}{n}\sum_{i=1}^n\left\|\boldsymbol{y}_i-(cR\boldsymbol{x}_i+\boldsymbol{t})\right\|^2\tag{1}$$&lt;p>维度$m$通常是2或者3。&lt;/p>
&lt;blockquote>
&lt;p>      This problem is sometimes called the absolute orientation problem ［1］, and an iterative algorithms for finding the solution ［2］ and a noniterative algorithm based on quaternions ［3］ are proposed for a 3-D problem. A good reference can be found in ［1］. Recently, Arun &lt;em>et al.&lt;/em> ［4］ and Horn &lt;em>et al.&lt;/em> ［1］ have presented a solution of this problem, which is based on the singular value decomposition of a covariance matrix of the data. Their solution, however, sometimes fails to give a correct rotation matrix and gives a reflection instead (det(R) = -1) when the data is severely corrupted.&lt;/p>
&lt;/blockquote>
&lt;p>      该问题有时被称为绝对定向问题，针对三维问题已经提出了迭代算法和基于四元数的非跌点算法来求解。关于这一问题，可以参考文献1。最近，Arun等人和Horn等人提出了一种基于数据协方差矩阵奇异值分解的解决方案。然而，当数据严重损坏时，他们的解决方案有时无法给出正确的旋转矩阵，而是给出一个反射矩阵（det(R) = -1）。&lt;/p>
&lt;blockquote>
&lt;p>      The theorem given in this correspondence is a strict solution of the problem, and it is derived by refining Arun&amp;rsquo;s result. The theorem always gives the correct transformation parameters even when the data is corrupted.&lt;/p>
&lt;/blockquote>
&lt;p>      该文中给出的定理是通过对 Arun 的结果进行改进而得到的问题的严格解。即使数据遭到破坏，该定理也总是能够给出正确的变换参数。&lt;/p>
&lt;h3 id="变换参数的最小二乘估计">变换参数的最小二乘估计
&lt;/h3>&lt;blockquote>
&lt;p>      In this section, we show a theorem which gives the least-squares estimation of similarity transformation parameters between two point patterns. Before showing the theorem, we prove a lemma, which gives the least-squares estimation of rotation parameters. This lemma is the main result of this correspondence.&lt;/p>
&lt;/blockquote>
&lt;p>在本节中，我们将展示一个定理，该定理给出了两个点模式之间相似变换参数的最小二乘估计。在展示定理之前，我们先证明一个引理，该引理给出了旋转参数的最小方差估计。这个引理是本文的主要结果。&lt;/p>
&lt;blockquote>
$$\min_R\left\|A-RB\right\|^2=\left\|A\right\|^2+\left\|B\right\|^2-2\mathrm{tr}(DS)\tag{2}$$$$S=\left\{
\begin{array}
{ll}I &amp; \quad\mathrm{if~}\det(AB^T)\geq0 \\
\operatorname{diag}(1,1,\cdots,1,-1) &amp; \quad\mathrm{if~}\det(AB^T)&lt;0.
\end{array}\right.\tag{3}$$$$R=USV^T\tag{4}$$$$S=\left\{
\begin{array}
{ll}I &amp; \quad\mathrm{if}\quad\det(U)\det(V)=1 \\
\operatorname{diag}(1,1,\cdots,1,-1) &amp; \quad\mathrm{if}\quad\det(U)\det(V)=-1
\end{array}\right.\tag{5}$$&lt;p>when $det (AB^T) = 0 (rank(AB^T) = m - 1)$.&lt;/p>
&lt;/blockquote>
$$\min_R\left\|A-RB\right\|^2=\left\|A\right\|^2+\left\|B\right\|^2-2\mathrm{tr}(DS)\tag{2}$$$$S=\left\{
\begin{array}
{ll}I &amp; \quad\mathrm{if~}\det(AB^T)\geq0 \\
\operatorname{diag}(1,1,\cdots,1,-1) &amp; \quad\mathrm{if~}\det(AB^T)&lt;0.
\end{array}\right.\tag{3}$$$$R=USV^T\tag{4}$$$$S=\left\{
\begin{array}
{ll}I &amp; \quad\mathrm{if}\quad\det(U)\det(V)=1 \\
\operatorname{diag}(1,1,\cdots,1,-1) &amp; \quad\mathrm{if}\quad\det(U)\det(V)=-1
\end{array}\right.\tag{5}$$&lt;p>。&lt;/p></description></item><item><title>如何理解概率论中的“矩”</title><link>/p/state_estimation_for_robotics/2_03/</link><pubDate>Mon, 09 Sep 2024 11:12:48 +0800</pubDate><guid>/p/state_estimation_for_robotics/2_03/</guid><description>&lt;img src="/p/state_estimation_for_robotics/2_03/gaussian-normal-distribution-graph.png" alt="Featured image of post 如何理解概率论中的“矩”" />&lt;h2 id="概率论中的矩">概率论中的“矩”
&lt;/h2>&lt;h3 id="彩票的问题">彩票的问题
&lt;/h3>&lt;p>假设福利彩票，每一注两元钱，且中奖的概率分布如下：
&lt;img src="/p/state_estimation_for_robotics/2_03/graph1.png"
width="359"
height="402"
srcset="/p/state_estimation_for_robotics/2_03/graph1_hu7074458194076008173.png 480w, /p/state_estimation_for_robotics/2_03/graph1_hu11398514835930632876.png 1024w"
loading="lazy"
alt="概率分布图"
class="gallery-image"
data-flex-grow="89"
data-flex-basis="214px"
>&lt;/p>
&lt;p>其中，概率的“称”如下所示：
&lt;img src="/p/state_estimation_for_robotics/2_03/graph2.png"
width="613"
height="402"
srcset="/p/state_estimation_for_robotics/2_03/graph2_hu1581581583894811490.png 480w, /p/state_estimation_for_robotics/2_03/graph2_hu3775081096410037377.png 1024w"
loading="lazy"
alt="概率的称"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="365px"
>&lt;/p>
&lt;p>此时我们称量一下中奖500万元：
&lt;img src="/p/state_estimation_for_robotics/2_03/graph3.png"
width="613"
height="402"
srcset="/p/state_estimation_for_robotics/2_03/graph3_hu9993873231620659325.png 480w, /p/state_estimation_for_robotics/2_03/graph3_hu8003149627366197390.png 1024w"
loading="lazy"
alt="500万元"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="365px"
>&lt;/p>
&lt;p>上述结果表明：不确定的500万元等价于确定的0.5元。此时将所有的中奖概率刻画上去：
&lt;img src="/p/state_estimation_for_robotics/2_03/graph4.png"
width="613"
height="402"
srcset="/p/state_estimation_for_robotics/2_03/graph4_hu8029704338762354472.png 480w, /p/state_estimation_for_robotics/2_03/graph4_hu18134410058688724433.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="365px"
>&lt;/p>
$$1.5 = 5\times 10\% + 100\times0.5\% + 5000000\times0.000001\% $$&lt;p>
结果表明一张彩票成本两元，但是期望获得的收益为1.5元，每买一张都会亏损0.5元。&lt;/p>
&lt;h2 id="矩">“矩”
&lt;/h2>&lt;h3 id="一阶矩">一阶矩
&lt;/h3>&lt;pre>&lt;code>上述我们计算的就是概率的一阶矩，也就是期望（expectation/mean）。
&lt;/code>&lt;/pre>
$$
E[X]=\sum p_{i}x_{i}
$$&lt;p>含义如下：
&lt;img src="/p/state_estimation_for_robotics/2_03/graph5.png"
width="572"
height="402"
srcset="/p/state_estimation_for_robotics/2_03/graph5_hu18168430126999916666.png 480w, /p/state_estimation_for_robotics/2_03/graph5_hu8960376600484578208.png 1024w"
loading="lazy"
alt="”期望含义“"
class="gallery-image"
data-flex-grow="142"
data-flex-basis="341px"
>&lt;/p>
&lt;h3 id="二阶矩">二阶矩
&lt;/h3>$$
\Sigma=E[(X-\mu)^{2}]=\sum_{i}p_{i}(x_{i}-\mu)^{2}
$$&lt;h3 id="高阶矩">高阶矩
&lt;/h3>&lt;p>三阶矩称为偏度，四阶矩称为峰度。各有用途但是共同的特点为称量之后才能使用。&lt;/p>
&lt;h2 id="参考链接">参考链接
&lt;/h2>&lt;p>[1]&lt;a class="link" href="https://matongxue.blog.csdn.net/article/details/109766892" target="_blank" rel="noopener"
>如何理解概率论中的“矩”？&lt;/a>&lt;/p></description></item><item><title>舒尔补</title><link>/p/state_estimation_for_robotics/2_02/</link><pubDate>Mon, 02 Sep 2024 23:51:06 +0800</pubDate><guid>/p/state_estimation_for_robotics/2_02/</guid><description>&lt;img src="/p/state_estimation_for_robotics/2_02/gaussian-normal-distribution-graph.png" alt="Featured image of post 舒尔补" />&lt;h2 id="舒尔补定义">舒尔补定义
&lt;/h2>&lt;p>给定任意的矩阵块 $M$ ，如下所示：&lt;/p>
$$M=\begin{bmatrix} A &amp; B\\ C &amp;D\\ \end{bmatrix}$$&lt;ul>
&lt;li>如果，矩阵块 $D$ 是可逆的，则 $A − B D^{-1} C$称之为 $D$ 关于 $M$的舒尔补。&lt;/li>
&lt;li>如果，矩阵块 $A$ 是可逆的，则 $D − CA^{-1} B$称之为 $A$ 关于 $M$的舒尔补。&lt;/li>
&lt;/ul>
&lt;h2 id="舒尔补的定理推导">舒尔补的定理推导
&lt;/h2>$$\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; B\\ C &amp; D\end{bmatrix}=\begin{bmatrix}A &amp; B\\ 0 &amp; \Delta _{A}\end{bmatrix}$$$$\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}
\begin{bmatrix}I &amp; -A^{-1}B\\ 0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; 0\\ C &amp; \Delta _{A}\end{bmatrix}$$$$\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; B\\ C &amp; D\end{bmatrix}
\begin{bmatrix}I &amp; -A^{-1}B\\ 0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; 0\\ 0 &amp; \Delta _{A}\end{bmatrix}
$$$$\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; 0\\ 0&amp; \Delta _{A}\end{bmatrix}
\begin{bmatrix}I &amp; A^{-1}B\\ 0&amp;I\\\end{bmatrix}=
\begin{bmatrix}A &amp; B\\ C &amp; D\ \end{bmatrix}
$$&lt;h2 id="用途">用途
&lt;/h2>&lt;h3 id="快速求矩阵的逆">快速求矩阵的逆
&lt;/h3>$$M=\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}=\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}
\begin{bmatrix}A &amp; 0\\ 0&amp; \Delta _{A}\end{bmatrix}
\begin{bmatrix}I &amp; A^{-1}B\\ 0&amp;I\\\end{bmatrix}$$$$M^{-1}=\begin{bmatrix}A &amp; B\\ C&amp;D\end{bmatrix}^{-1}=\begin{bmatrix}I &amp; 0\\ CA^{-1} &amp;I\\\end{bmatrix}^{-1}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; A^{-1}B\\ 0&amp;I\\\end{bmatrix}^{-1}\\ =\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; -A^{-1}B\\ 0&amp;I\\\end{bmatrix}\\=\begin{bmatrix}A^{-1}+A^{-1}B{\Delta _A}^{-1}CA^{-1}&amp;-A^{-1}B{\Delta _A}^{-1}\\-{\Delta _A}^{-1}CA^{-1}&amp;{\Delta _A}^{-1} \end{bmatrix}
$$&lt;h3 id="舒尔补在信息矩阵求解中的使用">舒尔补在信息矩阵求解中的使用
&lt;/h3>$$\sum=\begin{bmatrix}A &amp; C^{T}\\C&amp;D\end{bmatrix}$$$$\sum^{-1}=\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\\=\begin{bmatrix}A^{-1}+A^{-1}C^{T}{\Delta _A}^{-1}CA^{-1}&amp;-A^{-1}C^{T}{\Delta _A}^{-1}\\-{\Delta _A}^{-1}CA^{-1}&amp;{\Delta _A}^{-1} \end{bmatrix}\\\stackrel{\triangle}{=}\begin{bmatrix}\Lambda_{aa}&amp;\Lambda _{ab}\\\Lambda _{ba}&amp;\Lambda _{bb} \end{bmatrix}$$&lt;p>
其中，由上式可推导得$A^{-1}=\Lambda _{aa}-\Lambda _{ab}\Lambda _{bb}^{-1}\Lambda _{ba}$, 以及$D^{-1}=\Lambda _{bb}-\Lambda _{ba}\Lambda _{aa}^{-1}\Lambda _{ab}$，它们即为下次优化会使用的先验信息矩阵（边际概率的信息矩阵）。&lt;/p>
&lt;h3 id="通过舒尔补分解多元高斯分布">通过舒尔补分解多元高斯分布
&lt;/h3>$$ x=\begin{bmatrix}a\\b\end{bmatrix} $$$$\sum=\begin{bmatrix} A &amp; C^{T} \\C&amp;D \end{bmatrix}$$$$P(a,b)=P(a)P(b|a)\propto exp(-\frac{1}{2} \begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\begin{bmatrix}a&amp;b\end{bmatrix})$$$$ \begin{align}
P(a,b) \\
\propto exp\left ( -\frac{1}{2}\begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}A&amp;C^{T}\\C&amp;D\end{bmatrix}^{-1}\begin{bmatrix}a&amp;b\end{bmatrix}\right) \\
\propto exp \left( -\frac{1}{2}\begin{bmatrix}a\\b\end{bmatrix}^{T}\begin{bmatrix}I &amp; 0\\ -CA^{-1} &amp;I\\\end{bmatrix}\begin{bmatrix}A^{-1} &amp; 0\\ 0&amp; \Delta _{A}^{-1}\end{bmatrix}\begin{bmatrix}I &amp; -A^{-1}B\\ 0&amp;I\\\end{bmatrix}\begin{bmatrix}a&amp;b\end{bmatrix})\right )\\
\propto exp\left( -\frac{1}{2}\begin{bmatrix}a^{T}&amp;(b-CA^{-1}a)^{T}\end{bmatrix}\begin{bmatrix}A^{-1}&amp;0\\0&amp;{\Delta _A^{-1}}\end{bmatrix}\begin{bmatrix}a\\b-CA^{-1}a\end{bmatrix}\right)\\
\propto exp \left( -\frac{1}{2}(a^TA^{-1}a)+(b-CA^{-1}a)^{T} \Delta _A^{-1}(b-CA^{-1}a) \right)\\
\propto exp \left( -\frac{1}{2}a^{T}A^{-1}a\right)exp \left ( -\frac{1}{2}(b-CA^{-1}a)^{T}\Delta _A^{-1}(b-CA^{-1}a)\right)\\
\propto P(a)P(b|a)
\\\end{align}$$&lt;p>
在《机器人学中的状态估计》2.2.3章节&amp;quot;联合概率密度函数，分解与推断&amp;quot;可见相似内容,其实就是高斯推断，套用相关模型$P(a)$是观测（边际概率），$P(b|a)$是后验概率，$P(a|b)$是传感器模型。&lt;/p></description></item><item><title>边缘化</title><link>/p/state_estimation_for_robotics/2_01/</link><pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate><guid>/p/state_estimation_for_robotics/2_01/</guid><description>&lt;img src="/p/state_estimation_for_robotics/2_01/gaussian-normal-distribution-graph.png" alt="Featured image of post 边缘化" />&lt;h2 id="从优化角度理解边缘化">从优化角度理解边缘化
&lt;/h2>&lt;h3 id="模型">模型
&lt;/h3>$$\begin{bmatrix}H_{mm}&amp;H_{mr}\\H_{rm}&amp;H_{rr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}b_m\\b_r\end{bmatrix} $$&lt;p>
拆解的目的是通过边缘化将$X_m$从状态量里删除掉，但是要保留它的约束。在划窗模式里，这个$X_m$为要边缘化掉的量。&lt;/p>
&lt;h3 id="过程">过程
&lt;/h3>$$\begin{bmatrix}I&amp;0\\-H_{rm}H_{mm}^{-1}&amp;I\end{bmatrix}\begin{bmatrix}H_{mm}&amp;H_{mr}\\H_{rm}&amp;H_{rr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}I&amp;0\\-H_{rm}H_{mm}^{-1}&amp;I\end{bmatrix}\begin{bmatrix}b_m\\b_r\end{bmatrix}$$$$\begin{bmatrix}H_{mm}&amp;H_{mr}\\0&amp;H_{rr}-H_{rm}H_{mm}^{-1}H_{mr}\end{bmatrix}\begin{bmatrix}X_m\\X_r\end{bmatrix}=\begin{bmatrix}b_m\\b_r-H_{rm}H_{mm}^{-1}b_m\end{bmatrix}$$$$(H_{rr}-H_{rm}H_{mm}^{-1}H_{mr})X_r=b_r-H_{rm}H_{mm}^{-1}b_{m}$$&lt;p>
意义：此时可以不依赖$X_m$求解出$X_r$,若我们只关心$X_r$的值，则可以把$X_m$从模型中删除。&lt;/p>
&lt;h2 id="从滤波角度理解边缘化">从滤波角度理解边缘化
&lt;/h2>&lt;h3 id="模型-1">模型
&lt;/h3>$$\begin{align}
x_k=A_{k-1}x_{k-1}+v_k+w_k,&amp;k=1...K\\
y_k=C_kx_k+n_k,&amp;k=0...K
\end{align}$$&lt;h3 id="map估计角度">MAP估计角度
&lt;/h3>$$\hat{x}=arg\underset{x}{min} J(x)$$$${\frac{\partial J(x)}{\partial x^{T}}}|_x=-H^{T}W^{-1}(z-H\hat{x})=0 \Rightarrow (H^{T}W^{-1}H)\hat{x}=H^{T}W^{-1}z$$&lt;p>
&lt;em>注：此时形式以及是接近优化角度的$HX=b$。&lt;/em>&lt;/p>
&lt;h3 id="滤波角度">滤波角度
&lt;/h3>$$ \{ \hat {x}_{k-1} ,\hat{P}_{k-1}\}$$$$ \{ \hat {x}_{k} ,\hat{P}_{k}\}$$$$ \{ \hat {x}_{k} ,\hat{P}_{k}\}$$$$ z=\begin{bmatrix}\hat{x}_{k-1}\\v_k\\y_k\end{bmatrix},H=\begin{bmatrix}I&amp;&amp;\\-A_{k-1}&amp;I\\&amp;&amp;C_{k}\end{bmatrix},W=\begin{bmatrix}\hat {P}_{k-1}&amp;&amp;\\&amp;Q_k&amp;\\&amp;&amp;R_k\end{bmatrix}$$$$(H_{k}^{T}W_{k}^{-1}H_{k})\hat{x}=H_{k}^{T}W_{k}^{-1}z_k$$$$\hat{x}=\begin{bmatrix}\hat{x'}_{k-1}\\\hat{x}_k\end{bmatrix}$$&lt;p>
&lt;img src="/p/state_estimation_for_robotics/2_01/image00.png"
width="714"
height="352"
srcset="/p/state_estimation_for_robotics/2_01/image00_hu2802948725242621933.png 480w, /p/state_estimation_for_robotics/2_01/image00_hu16496475951586850660.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="486px"
>
借助本文第一节，目标为从$x$变量中删除$\hat{x'}_{k-1}$，执行舒尔补可得
&lt;img src="/p/state_estimation_for_robotics/2_01/image01.png"
width="707"
height="463"
srcset="/p/state_estimation_for_robotics/2_01/image01_hu12842677403844977782.png 480w, /p/state_estimation_for_robotics/2_01/image01_hu10364658897043807725.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>
&lt;img src="/p/state_estimation_for_robotics/2_01/image02.png"
width="1371"
height="694"
srcset="/p/state_estimation_for_robotics/2_01/image02_hu7680983322299133859.png 480w, /p/state_estimation_for_robotics/2_01/image02_hu10029282904931644770.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="474px"
>&lt;/p></description></item></channel></rss>